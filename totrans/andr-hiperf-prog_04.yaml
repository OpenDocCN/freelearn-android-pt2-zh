- en: Chapter 4. Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When trying to reach the performance target for our application, memory is
    the matter to focus on: poorly managed memory in an application can affect the
    behavior of the whole system. It can also affect the other applications installed
    on our device in the same way as other applications could affect ours. As we all
    know, Android has a wide range of devices on the market with a lot of different
    configurations and memory amounts. It''s up to the developers to work out which
    strategy to take while dealing with this amount of fragmentation, which pattern
    to follow while developing, and which tools to use to profile the code. This is
    the aim of this chapter. We will focus on heap memory, while we deal with cache
    in [Chapter 10](ch10.html "Chapter 10. Performance Tips"), *Performance Tips*.'
  prefs: []
  type: TYPE_NORMAL
- en: We will have a look at how our device handles memory, deepening our knowledge
    of what garbage collection is and how it works in order to understand how to avoid
    common developing mistakes, and clarify what we will discuss to define best practices.
    We will also go through pattern definition in order to drastically reduce the
    risk of what we will identify as memory leaks and memory churns. This chapter
    will end with an overview of the official tools and APIs that Android provides
    to profile our code and to find possible causes of memory leaks that aren't covered
    in [Chapter 2](ch02.html "Chapter 2. Efficient Debugging"), *Efficient Debugging*.
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting the discussion about how to improve and profile our code, it's
    really important to understand how Android devices handle memory. Then, in the
    following pages we will analyze differences between the runtimes that Android
    uses, we will learn more about garbage collection, understand memory leaks and
    memory churns, and how Java handles object references.
  prefs: []
  type: TYPE_NORMAL
- en: How memory works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Have you ever thought about how a restaurant works? Let''s think about it for
    a while: when new groups of customers get into the restaurant there''s a waiter
    ready to search for a place to allocate them. But the restaurant is a limited
    space. So, there is a need to free tables when possible: that''s why, when a group
    has finished eating, another waiter cleans and prepares the table for other groups
    to use. The first waiter has to find the table with the right number of seats
    for every new group. Then, the second waiter''s task should be fast and shouldn''t
    hinder or block the others'' tasks. Another important aspect of this is how many
    seats are occupied by the group: the restaurant owner wants to have as many free
    seats as possible to seat new clients. So it''s important to make sure that every
    group fills the right number of seats without occupying tables that could be used
    by other new groups.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is similar to what happens in an Android system: every time we create
    a new object in our code, it needs to be saved in memory. So it''s allocated as
    part of our application''s private memory to be accessed whenever needed. And
    the system keeps allocating memory for us during the whole of our application''s
    lifetime. Nevertheless, the system has limited memory to use and it cannot allocate
    memory indefinitely. So, how is it possible for the system to have enough memory
    for our application all the time? And why is there no need for an Android developer
    to free up memory? Let''s find out.'
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Garbage collection is an old concept that is based on two main concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Find objects that are not referenced any more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free the memory referenced by those objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When that object is not referenced any more, its "table" can be cleaned and
    freed up. This is what is done to provide memory for future object allocation.
    These operations of the allocation of new objects and the deallocation of objects
    that are not referenced any more are executed by the particular runtime in use
    on the device, and there is no need for the developer to do anything just because
    they are all managed automatically. In spite of what happens in other languages,
    such as C and C++, there is no need for the developer to allocate and deallocate
    memory. In particular, while the allocation is made when needed, the garbage collection
    task is executed when an upper limit of memory is reached. Those automatic operations
    in the background don''t exempt developers from being aware of their app''s memory
    management: if the memory management is not done well, the application can be
    prone to lags, malfunctions, and even crashes when an `OutOfMemoryError` is thrown.'
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Android, every app has its own process that is completely managed by the
    runtime with the aim of reclaiming memory in order to free resources for other
    foreground processes, if necessary. The available amount of memory for our application
    lies completely in RAM as Android doesn''t use swap memory. The main consequence
    of this is that there is no other way for our app to have more memory than to
    unreference objects that are no longer used. But Android uses paging and memory-mapping:
    the first technique defines blocks of memory of the same size, called pages, in
    a secondary storage; while the second one uses a mapping in memory with correlated
    files in secondary storage to be used as primary. They are used when the system
    needs to allocate memory for other processes, so the system creates paged memory-mapped
    files to save Dalvik code files, app resources, or native code files. In this
    way, those files can be shared between multiple processes.'
  prefs: []
  type: TYPE_NORMAL
- en: As a matter of fact, Android uses shared memory in order to better handle resources
    from a lot of different processes. Furthermore, every new process to be created
    is forked by an already existing one that is called **Zygote**. This particular
    process contains common framework classes and resources to speed up the first
    boot of the application. This means that the Zygote process is shared between
    processes and applications. This large use of shared memory makes it difficult
    to profile the use of memory of our application because there are many facets
    to consider before reaching the correct analysis of memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some functions and operations of memory management depend on the runtime used.
    That''s why we are going through some specific features of the two main runtimes
    used by Android devices. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dalvik
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Android runtime** (**ART**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ART was added later to replace Dalvik to improve performance from a different
    point of view. It was introduced in Android KitKat (API Level 19) as an option
    for developers to enable, and it has become the main and only runtime from Android
    Lollipop (API Level 21) onwards. Besides the difference between Dalvik and ART
    in compiling code, file formats, and internal instructions, what we are focusing
    on at the moment is memory management and garbage collection. So, let's understand
    how the Google team improved performance in runtime garbage collection over time
    and what to pay attention to while developing our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s step back and return to the restaurant. What would happen if everything,
    all employees, such as other waiters and cooks, and all of the services, such
    as dishwashers, stop their tasks while they wait for a waiter to free a table?
    The success or failure of the whole restaurant relies on that single employee''s
    performance. So, it''s really important to have a very fast waiter in this case.
    But what do you do if you cannot afford him? The owner wants him to do what he
    has to as quickly as possible by maximizing his productivity and, then, allocating
    all the customers in the best way. And this is exactly what we have to do as developers:
    we have to optimize memory allocation in order to have a fast garbage collection,
    even if it stops all the other operations. What is described here is just how
    the runtime garbage collection works: when the upper limit of memory is reached,
    the garbage collection starts its task, pausing every other method, task, thread,
    and process execution. And those objects won''t resume until the garbage collection
    task is completed. So, it''s really important that the collection is fast enough
    to not impede the 16 ms per frame rule we discussed in [Chapter 3](ch03.html "Chapter 3. Building
    Layouts"), *Building Layouts*, resulting in lags and jank in the UI: the more
    time the garbage collection takes, the less time the system has to prepare frames
    to be rendered on the screen.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Keep in mind that automatic garbage collection is not free: bad memory management
    can lead to bad UI performance and, thus, a bad UX. No runtime feature can replace
    good memory management. That''s why we need to be careful about new allocations
    of objects and, above all, references.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, ART introduced a lot of improvement in this process after the Dalvik
    era, but the background concept is the same: it reduces the collection steps,
    it adds a particular memory for bitmap objects, it uses new fast algorithms, and
    it does other cool stuff that will get better in the future, but there is no way
    to escape that we need to profile our code and memory usage if we want our application
    to have the best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Android N JIT compiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ART runtime uses an ahead-of-time compilation that, as the name suggests,
    performs compilation when the applications are first installed. This approach
    brings advantages to the overall system in different ways because the system can
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce battery consumption due to pre-compilation and, then, improve autonomy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute applications faster than Dalvik
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve memory management and garbage collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, these advantages have a cost related to installation timing: the system
    needs to compile the application at that time, and then, it''s slower than a different
    type of compiler.'
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, Google added a just-in-time compiler to the ahead-of-time compiler
    of ART in the new Android N. This one acts when needed, so during the execution
    of the application and, then, it uses a different approach compared to the ahead-of-time
    one. This compiler uses code profiling techniques, and it's not a replacement
    for the ahead-of-time compiler, but it's an addition to it. It's a good enhancement
    to the system for the advantages in terms of performance that it introduces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Profile-guided compilation adds the possibility to precompile and then, to
    cache and reuse methods of the application, depending on usage and/or device conditions.
    This feature can save time in the compilation and improve performance in every
    kind of system. So, all devices benefit from this new memory management. The key
    advantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Less memory used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fewer RAM accesses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower impact on the battery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these advantages introduced in Android N, however, shouldn't make us
    avoid good memory management in our applications. For this, we need to know what
    pitfalls are lurking behind our code and, more than this, how to behave in particular
    situations to improve the memory management of the system while our application
    is active.
  prefs: []
  type: TYPE_NORMAL
- en: Memory leak
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main mistake, from the memory performance perspective, that a developer
    can make while developing an Android application is called **memory leak**, and
    it refers to an object that is no longer used but is referenced by another object
    that is still active. In this situation, the garbage collector skips it because
    the reference is enough to leave that object in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, we are avoiding that the garbage collector frees memory for other
    future allocations. So, our heap memory gets smaller because of this, and this
    leads to the garbage collection to be invoked more often, blocking the rest of
    the executions of the application. This could lead to a situation where there
    is no more memory to allocate a new object and, then, `OutOfMemoryError` is thrown
    by the system. Consider the case where a used object references objects that are
    no longer used, that references objects that are no longer used, and so on: none
    of them can be collected, just because the root object is still in use.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory churn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another anomaly in memory management is called **memory churn**, and it refers
    to the amount of allocations that are not sustainable by the runtime for too many
    newly instantiated objects in a small period of time. In this case, a lot of garbage
    collection events are called many times, affecting the overall memory and UI performance
    of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we discussed in [Chapter 3](ch03.html "Chapter 3. Building Layouts"),
    *Building Layouts*, regarding the need to avoid allocations in the `View.onDraw()`
    method, is closely related to memory churn: we know that this method is called
    every time the view needs to be drawn again and the screen needs to be refreshed
    every 16.6667 ms. If we instantiate objects inside that method, we could cause
    a memory churn because those objects are instantiated in the `View.onDraw()` method
    and no longer used, so they are collected very soon. In some cases, this leads
    to one or more garbage collection events to be executed every time the frame is
    drawn on the screen, reducing the available time to draw it below the 16.6667
    ms, depending on the duration of the collection event.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s have a quick overview of the different objects that Java provides us
    to reference objects. This way, we will have an idea of when we can use them and
    how Java defines four levels of strength:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normal**: This is the main type of reference. It corresponds to the simple
    creation of an object, and this object will be collected when it will no longer
    be used and referenced, and it''s just the classical object instantiation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Soft**: This is a reference that''s not strong enough to keep an object in
    memory when a garbage collection event is triggered, so it can be null any time
    during execution. Using this reference, the garbage collector decides when to
    free the object memory based on the memory demand of the system. To use it, just
    create a `SoftReference` object passing the real object as parameter in the constructor
    and call the `SoftReference.get()` to get the object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Weak**: This is like `SoftReferences`, but weaker:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Phantom**: This is the weakest reference; the object is eligible for finalization.
    This kind of reference is rarely used and the `PhantomReference.get()` method
    always returns null. This is for reference queues that don''t interest us at the
    moment, but it''s useful to know that this kind of reference is also provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These classes may be useful while developing if we know which objects have a
    lower level of priority and can be collected without causing problems in the normal
    execution of our application. We will see how they can help us manage memory in
    the following pages.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-side projects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During the development of the Android platform, Google has always tried to
    improve the memory management system of the platform to maintain wide compatibility
    with increasing performance devices and low resource ones. This is the main purpose
    of two projects Google develops in parallel with the platform, and, then, every
    new Android version released means new improvements and changes to those projects
    and their impacts on the system performance. Every one of these side projects
    focuses on a different matter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Project Butter**: This was introduced in Android Jelly Bean 4.1 (API Level
    16) and then improved in Android Jelly Bean 4.2 (API Level 17); it added features
    related to the graphical aspect of the platform (VSync and buffering are the main
    additions) in order to improve the responsiveness of the device while in use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Project Svelte**: This was introduced in Android KitKat 4.4 (API Level 19);
    it deals with memory management improvements in order to support low RAM devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Project Volta**: This was introduced in Android Lollipop (API Level 21);
    it focuses on the battery life of the device. Then, it adds important APIs to
    deal with batching expensive battery draining operations, such as the JobScheduler,
    or new tools such as the Battery Historian.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project Svelte and Android N
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it was first introduced, Project Svelte reduced the memory footprint and
    improved the memory management in order to support entry-level devices with low
    memory availability and then broadened the supported range of devices with clear
    advantages for the platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the release of Android N, Google wants to provide an optimized way to
    run applications in the background. We know that the process of our application
    runs in the background even if it is not visible on the screen, and even if there
    are no running activities, because a service could be executing some operations.
    This is a key feature for memory management: the overall system performance could
    be affected by bad memory management of the background processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what''s changed in the application behavior and the APIs with the new Android
    N? The chosen strategy to improve memory management, reducing the impact of background
    processes, is to avoid sending the application broadcasts for the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ConnectivityManager.CONNECTIVITY_ACTION`: Starting from Android N, a new connectivity
    action will just be received from those applications that are in the foreground
    and that have a registered `BroadcastReceiver` for this action. No application
    with an implicit intent declared inside the manifest file will receive it any
    longer. Hence, the application needs to change its logic to do the same as before.
    [Chapter 6](ch06.html "Chapter 6. Networking"), *Networking*, deals with this,
    so refer to that chapter to learn more about this particular topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Camera.ACTION_NEW_PICTURE`: This is used to notify that a picture has just
    been taken and added to the media store. This action won''t be available any more,
    neither for receiving nor for sending, and it will be for any application, not
    just for the ones that are targeting the new Android N.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Camera.ACTION_NEW_VIDEO`: This is used to notify that a video has just been
    taken and added to the media store. As with the previous one, this action cannot
    be used any more, and it will be for any application too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind these changes when targeting the application with the new Android
    N to avoid unwanted or unexpected behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: All of the actions listed have been changed by Google to force developers not
    to use them in applications. As a general rule, we should not use implicit receivers
    for the same reason. Hence, we should always check the behavior of our application
    while it's in the background because this could lead to unexpected memory usage
    and battery drain. Implicit receivers can start our application components, while
    the explicit ones are set up for a limited time while the activity is in the foreground,
    and then they cannot affect the background processes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's a good practice to avoid the use of implicit broadcasts while developing
    applications to reduce the impact of it on background operations that could lead
    to unwanted waste of memory and, then, a battery drain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, Android N introduces a new command in ADB to test the application''s
    behavior of ignoring the background processes. Use the following command to ignore
    background services and processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following command to restore the initial state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Refer to [Chapter 5](ch05.html "Chapter 5. Multithreading"), *Multithreading*,
    to understand how processes work on an Android device.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what can happen in memory while our application is active,
    let's examine what we can do to avoid memory leaks and memory churns and optimize
    our memory management in order to reach our performance target, not just in memory
    usage, but in garbage collection attendance, because, as we know, it stops any
    other operation from working.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following pages, we will go through a lot of hints and tips using a
    bottom-up strategy: starting from low-level shrewdness in Java code to highest-level
    Android practices.'
  prefs: []
  type: TYPE_NORMAL
- en: Data types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We weren''t joking: we are really talking about Java primitive types, as they
    are the foundation of all the applications and it''s really important to know
    how to deal with them, even though it may be obvious. It''s not, and we will soon
    understand why.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Java provides primitive types that need to be saved in memory when used: the
    system allocates an amount of memory related to the amount requested for that
    particular type. The following are Java primitive types with the related amount
    of bits needed to allocate the type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`byte`: 8 bits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`short`: 16 bits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int`: 32 bits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`long`: 64 bits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float`: 32 bits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`double`: 64 bits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boolean`: 8 bits, but it depends on the virtual machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`char`: 16 bits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At first glance, what is clear is that you should be careful when choosing the
    right primitive type every time you are going to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Don''t use a bigger primitive type if you don''t really need it: never use
    `long`, `float`, or `double` if you can represent the number with an integer.
    It would be a useless waste of memory and calculations every time the CPU needs
    to deal with it. And remember that to calculate an expression, the system needs
    to do a widening primitive implicit conversion to the largest primitive type involved
    in the calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: Autoboxing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Autoboxing is the term used to indicate an automatic conversion between a primitive
    type and its corresponding wrapper class object. Primitive type wrapper classes
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`java.lang.Byte`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.Short`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.Long`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.Float`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.Boolean`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.Character`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They can be instantiated using the assignment operator as for the primitive
    types and they can be used as their primitive types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This is exactly the same as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'But the use of autoboxing is not the right way to improve the performance of
    our applications. There are many costs associated with it: first of all, the wrapper
    object is much bigger than the corresponding primitive type. For instance, an
    `Integer` object needs 16 bytes in memory instead of 16 bits for the primitive
    type. Hence, more memory is used to handle it. Then, when we declare a variable
    using the primitive wrapper object, any operation on that implies at least another
    object allocation. Have a look at the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Every Java developer knows what it is, but this simple code needs an explanation
    of what happened step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, the integer value is taken from the `Integer` value `integer`
    and it''s increased by 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, the result is assigned to the integer, but this means that a new autoboxing
    operation needs to be executed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Undoubtedly, these operations are slower than if we used the primitive type
    instead of the wrapper class: there''s no need for autoboxing, hence, no more
    bad allocations. Things can get worse in loops, where the preceding operations
    are repeated every cycle. Take, for example, the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, there are a lot of inappropriate allocations caused by autoboxing
    and if we compare this with the primitive type `for` loop, we notice that there
    are no allocations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Autoboxing should be avoided as much as possible: the more we use primitive
    wrapper classes instead of primitive types, the more wasted memory there will
    be while executing our application. And this waste could be propagated when using
    autoboxing in loop cycles, affecting not just memory, but CPU timings as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse array family
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So, in all of the cases described in the previous paragraph, we can just use
    the primitive type instead of the object counterpart. Nevertheless, it''s not
    always so simple. What happens if we are dealing with generics? For example, let''s
    think about collections: we cannot use primitive types as generics for objects
    that implement one of the following interfaces. We have to use the wrapper class
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Every time we use one of the `Integer` objects of a collection, autoboxing
    occurs at least once, producing the preceding waste outlined. And we well know
    how many times we deal with this kind of object in everyday developing time. But
    isn''t there a solution to avoid autoboxing in these situations? Android provides
    a useful family of objects created to replace `Map` objects and avoid autoboxing,
    protecting memory from pointlessly large allocations: they are the Sparse arrays.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of Sparse arrays, with the related type of maps they can replace, is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SparseBooleanArray: HashMap<Integer, Boolean>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparseLongArray: HashMap<Integer, Long>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparseIntArray: HashMap<Integer, Integer>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparseArray<E>: HashMap<Integer, E>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LongSparseArray<E>: HashMap<Long, E>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section we will talk about the `SparseArray` object specifically,
    but everything we say is true for all of the previously mentioned objects as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SparseArray` object uses two different arrays to store hashes and objects.
    The first one collects sorted hashes, while the second one stores the key-value
    pairs ordered according to the key hashes array sorting in *Figure 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sparse array family](img/4666_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: SparseArray''s hashes structure'
  prefs: []
  type: TYPE_NORMAL
- en: When you need to add a value, you have to specify the integer key and the value
    to be added in the `SparseArray.put()` method, just like in `HashMap`. This could
    create collisions if multiple key hashes are added to the same position.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a value is needed, simply call `SparseArray.get()`, specifying the related
    key: internally, the key object is used to binary search the index of the hash
    and then, the value of the related key, as in *Figure 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sparse array family](img/4666_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: SparseArray''s workflow'
  prefs: []
  type: TYPE_NORMAL
- en: When the key found in the index resulting from the binary search does not match
    the original one, a collision happened, so the search keeps on in both directions
    to find the same key and to provide the value, if it's still inside the array.
    Thus, the time needed to find the value increases significantly if a large number
    of objects is contained by the array.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, a `HashMap` contains just a single array to store hashes, keys,
    and values, and it uses large arrays as a technique to avoid collisions. This
    is not good for memory, because it's allocating more memory than is really needed.
    So `HashMap` is fast, because it implements a better way of avoiding collisions,
    but it's not memory efficient. Conversely, `SparseArray` is memory efficient because
    it uses the right number of object allocations, with an acceptable increase in
    execution timing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory used for these arrays is contiguous, so every time you remove a
    key/value pair from a `SparseArray`, they can be compacted or resized:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compaction**: The object to remove is shifted to the end and all the other
    objects are shifted left. The last block containing the item to be removed can
    be reused for future additions to save allocations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resize**: All the elements of the arrays are copied to other arrays and the
    old ones are deleted. On the other hand, the addition of new elements produces
    the same effect as copying all the elements into new arrays. This is the slowest
    method, but it''s completely memory safe because there are no useless memory allocations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, `HashMap` is faster while doing these operations because it contains
    more blocks than is really needed, hence the memory waste.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The use of `SparseArray` family objects depends on the strategy of memory management
    and CPU performance patterns being used because of the calculation performance
    cost compared to the memory saving. So, the use is right in some situations. Consider
    the use of it when:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of objects you are dealing with is below a thousand and you are not
    going to do a lot of additions and deletions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are using collections of maps with few items, but lots of iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another useful feature of these objects is that they let you iterate over indexing,
    instead of using the iterator pattern, which is slower and memory inefficient.
    The following snippet shows how the iteration doesn''t involve objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Contrariwise, the `Iterator` object is needed to iterate through `HashMaps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Some developers think the `HashMap` object is the better choice because it can
    be exported from an Android application to other Java ones, while the `SparseArray`
    family's objects don't. But what we have analyzed here as memory management gains
    is applicable to any other case. And, as developers, we should strive to reach
    performance goals on every platform, instead of reusing the same code on different
    platforms, because different platforms could be affected differently from a memory
    perspective. That's why our main suggestion is to always profile the code in every
    platform we are working on, and then make our own personal considerations on the
    best and worst approaches, depending on results.
  prefs: []
  type: TYPE_NORMAL
- en: ArrayMap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An `ArrayMap` object is an Android implementation of the `Map` interface that
    is more memory efficient than the `HashMap` one. This class is provided by the
    Android platform starting from Android KitKat (API Level 19), but there is another
    implementation of this inside the support package v4 because of its main usage
    on older and low-end devices.
  prefs: []
  type: TYPE_NORMAL
- en: Its implementation and usage is similar to the `SparseArray` objects with all
    the implications about memory usage and computational costs, but its main purpose
    is to let you use `Objects` as keys of the map, just like `HashMap` does. Hence,
    it provides the best of both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, we are not careful enough with the simple and common Java structures
    we use every day in Android application development. But are we sure those basic
    Java syntaxes are always suitable for performance? Let's find out.
  prefs: []
  type: TYPE_NORMAL
- en: Collections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We dealt with collections in the previous paragraph. We now want to face the
    implications of iteration over a collection to detect the best choice to iterate
    objects inside a collection and, then, improve memory management. Let''s compare
    timing results of three different cycles:'
  prefs: []
  type: TYPE_NORMAL
- en: The `Iterator` cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `while` cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `for` cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have used the following snippet of code to compare their timings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We tested ten times the performance of the loops using different number of items
    in the list and we averaged the measurements. The results of these measurements
    are in *Figure 3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Collections](img/4666_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Memory statistics results for the cycle measurements'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results can vary depending on a lot of different factors: memory, CPU,
    running applications on the device, and so on. But what we are interested in is
    finding the average performance for these cycles. What is evident from the graph
    is that the `Iterator` cycle type is the slowest one, while the `for` cycle is
    always the fastest in our measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, is there just a single way to create a `for` cycle? No, there are different
    alternative. Let''s see them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first case is the slowest one, because there is an extra cost due to array
    length calculations in every cycle, because the just-in-time compilation needs
    to translate it every time. The second case avoids this cost by calculating the
    length just once, while the last one is the enhanced `for` loop syntax introduced
    with Java 5, which is the fastest way to index using the `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enhanced `for` loop syntax is the fastest way to index over an array, even if
    the device has a just-in-time compilation, so consider it every time you deal
    with array iterations and avoid iterations with the `iterator` object as much
    as possible, as it is the slowest one.
  prefs: []
  type: TYPE_NORMAL
- en: Enumerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Enumerations are very comfortable for developers: a limited number of elements,
    descriptive names, and therefore improved code readability. They also support
    polymorphism. For these reasons they are widely used in our code. But are they
    really good performance-wise? The main alternative to an enumeration is the declaration
    of integers that are publicly accessible and static. For example, let''s have
    a look at the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'That can be replaced by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, which one is more expensive from a memory perspective? The answer to this
    question is twofold: we can check the DEX size produced for our app that, then,
    affects the heap memory usage during execution with enumerations or with integer
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: Our example enumeration is converted into four objects allocation with `String`
    for the name and an `integer` value as the ordinal, and an `array` and the wrapper
    class. Instead, the class implementation is light because it just allocates the
    four integer values with a considerable saving of memory.
  prefs: []
  type: TYPE_NORMAL
- en: To make matters worse, the enumeration needs to be replicated in every process
    your app is using, so, its costs increase in a multiprocess application.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the classic usage of an enumeration, a `switch...case` statement is needed,
    so let''s look at it using our enumeration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, let''s change the previous code using the integer values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This kind of change in our code is pretty simple. So, we should think of planning
    to reformat our code to reduce or remove used enumerations, due to our previous
    reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Android provides a useful annotation to simplify the transition from enumeration
    to integer values: `@IntDef`. This annotation can be used to enable multiple constants
    by using the `flag` attribute in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This annotation says that the possible values are those specified inside the
    annotation itself. For example, let''s change our integer values to use the annotation
    and transform those values to something similar to an enumeration without all
    the memory performance issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to use it in our code, simply specify the new annotation where you are
    expecting to have a `Shape` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enumerations affect the overall memory performance because of their unneeded
    allocations. Then, avoid using them and swap as many as possible with static final
    integer values. Then create your own annotation to use those integer values as
    if they were an enumeration, just to have a limited number of values.
  prefs: []
  type: TYPE_NORMAL
- en: In some situations, you cannot remove your enumerations. Nevertheless, Proguard
    can be enhanced to decrease the impact of enumerations on our application memory
    performance. Refer to [Chapter 10](ch10.html "Chapter 10. Performance Tips"),
    *Performance Tips*, to learn more about this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Constants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, we need a variable that is not related to a particular instance of a
    class, but that is used all over the application. Yes, we are talking about static
    variables. They are useful in a lot of situations. But how are they managed by
    the system? What memory implications are behind this? Let's step back and talk
    about how the compiler handles static variables during execution. There is a special
    method in the Java compiler called `<clinit>`. As the name suggests, it deals
    with class initializations, but it's just used for variables and static blocks
    and it initializes them in the order they are inside the class. It's executed
    starting from the class's super-classes and interfaces, down to the class itself.
    So, our static variables are initialized as soon as the application starts.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a different perspective if the static variables are final as well: in
    this case, they aren''t initialized by the `<clinit>` method, but they are stored
    inside the DEX file with double benefits. They don''t need either more memory
    allocations, nor the operations to allocate it. This only applies to primitive
    types and string constants, so there''s no need to do it for objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constants in the code should be static and final in order to take advantage
    of memory savings and to avoid their initialization in the Java compiler `<clinit>`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Object management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at a higher Java topic, covering the correct management of objects
    and some practices to avoid memory pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with banalities that aren''t actually so trivial: be careful not
    to instantiate unnecessary objects. We never tire of saying it. Memory allocations
    are expensive, and deallocations are too: the system allocates memory for it and
    the garbage collection limit is reached sooner, and, as we know, this slows down
    the overall application performance from memory availability to lags in the user
    experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every developer should know and do this task of cleaning up unnecessary objects
    in the code. There is no absolute rule for this: just keep in mind that a few
    useful objects are more memory safe than a lot of rarely used ones.'
  prefs: []
  type: TYPE_NORMAL
- en: Create fewer temporary objects, as they are often garbage collected, and avoid
    instantiating unnecessary objects, as they are expensive for memory and computational
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The following pages are rich with simple practices to follow in order to limit
    as much as possible the memory consumption of our application so it will never
    fall into lags. We want to deal with Java techniques for object management in
    the next paragraphs, while we will present later on the methodologies related
    to Android. They are, however, related to common situations for Android developers.
  prefs: []
  type: TYPE_NORMAL
- en: Strings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `String` objects are immutable. When you instantiate a string this way,
    you are forcing the allocation of two different objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The two objects are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `String` `"example"`, which is an object itself, and its memory must be
    allocated anyway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new `String string`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, the other initialization of a `String` object is much more suitable for
    memory performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: String concatenation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Often, we use strings and manipulate them with no thought of the aftermath
    in memory. One would think that when we need to concatenate two or more strings,
    the following snippet would be good for memory performance because it doesn''t
    use more object allocations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'But instead, for this kind of operation, `StringBuffer` and `StringBuilder`
    are more efficient than the `String` class because they work on character arrays.
    Then, for a better execution, the previous snippet should be changed into the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is preferable if you work a lot with string concatenation, but it can
    be used as a good practice all the time, just because of the higher efficiency
    of `StringBuffer` and `StringBuilder` compared to string concatenations. Remember
    the difference between `StringBuffer` and `StringBuilder`: the first one is thread
    safe, so it''s slower, but it can be used in a multithreading environment; while
    `StringBuilder` is not thread safe, so it''s faster, but it can only be used in
    a single thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing to keep in mind is that both `StringBuilder` and `StringBuffer`
    have an initial capacity of 16 characters, and when they need to be increased
    because of full capacity, a new object with double capacity is instantiated and
    allocated and the old one is waiting for the next garbage collection to be done.
    To avoid this unnecessary waste of memory, if you know an estimation of the string
    capacity you are dealing with, you can instantiate `StringBuffer` or `StringBuilder`
    by specifying a different initial capacity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This way, no object recreation is needed if the string capacity is lower than
    64 characters and it will not be collected until it's no longer referenced.
  prefs: []
  type: TYPE_NORMAL
- en: Local variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Looking at our code, sometimes we notice that an object used in a method is
    used without being modified for all of the method execution. This means that it
    can be exported outside the method, so it''s allocated once and never collected,
    improving memory management. For example, the next code suggests just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the `DateFormat` object doesn''t need to be instantiated every
    time the method is executed. Furthermore, a new object is allocated every time
    and it''s not collected until the garbage collector limit is reached, occupying
    memory unnecessarily in the meantime. It would be much better to extract that
    object from the method and make it available from the outside, so that it''s only
    instantiated once and it''s available throughout the life cycle of the `class`
    object. The overall performance benefit would come from the reuse of a single
    object in multiple places where a `DateFormate.format()` method call is needed.
    Then, a solution could be used, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In general, there are a lot of different situations where you need to handle
    local variables that could be extracted and there are lots of different solutions:
    it''s up to you to find the one that fits your code well.'
  prefs: []
  type: TYPE_NORMAL
- en: Arrays versus collections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Collections can be automatically enlarged or reduced in need and provide a
    lot of helpful methods to add, remove, get, change, and move objects, and other
    cool things. This comes with a high cost. If the number of objects you are dealing
    with is fixed, raw arrays are more memory efficient than collections. The [http://bigocheatsheet.com](http://bigocheatsheet.com)
    website reports a deeper analysis about cost comparison between arrays and collections.
    For this purpose, the Big O notation is used: it describes the trend of the algorithm
    to the growth of the number of elements of the array/collection.'
  prefs: []
  type: TYPE_NORMAL
- en: Streams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common error made while dealing with I/O stream Java objects is to not release
    and free them properly, or to not free them at all, with obvious consequent memory
    leaks. Remember to release them every time, because this mistake can affect overall
    performance. Let''s look at the following sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code for releasing is the incorrect one. Many developers use
    it, but there''s still a source of memory leak. If an exception is thrown while
    closing `InputStream`, `OutputStream` is not closed and it remains referenced,
    causing the memory leak mentioned earlier. The following snippet shows how to
    handle it correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As a general rule, you should always use the `finally` keyword in the `try...catch`
    statements to free resources and memory and close every closable object separately
    from other ones.
  prefs: []
  type: TYPE_NORMAL
- en: Memory patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will have a look at a couple of useful design patterns that
    can decrease the risk of memory churn, if well handled, or limit the memory used
    for the objects used. Their aim is to reduce the memory allocations if a lot of
    objects are about to be used. They reduce the garbage collector calls as well.
    The choice of whether to use them depends on the particular situation, the requirements,
    and the expertise of the developer. They can be very useful, but if you use them,
    it's really important that you are careful about memory leaks that you may introduce
    that could nullify the effects of their use.
  prefs: []
  type: TYPE_NORMAL
- en: The object pool pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Among creational design patterns, the object pool pattern is really helpful
    to reuse already allocated objects and then reaching the goal of avoiding memory
    churn and all of its possible side effects on the application performance. It's
    particularly useful when we are dealing with expensive creation objects and we
    need to create a lot of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind this is to avoid garbage collection on an object that can be
    reused for future needs and to save time creating it. To get to this, an object
    called `ObjectPool` handles many reusable objects, making them available to those
    who request them. These requesting objects are called **clients**. So, this patterns
    deals with three kinds of objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ReusableObject`: These are objects that can be made available for clients
    and that are handled by the pool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Client`: This is the object that needs a reusable object to do some stuff,
    so it has to ask the pool and it has to return it once the stuff is completed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ObjectPool`: This holds every reusable object in order to provide and regain
    every single one of them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ObjectPool` should be a singleton object in order to have a centralized management
    of all the reusable objects, to avoid confusing exchanges between different pools
    and to share a correct and consistent policy approach for every reusable object''s
    creation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pool can have an upper limit for the number of contained objects. This
    means that if a client is requesting a reusable object and the pool is full and
    doesn''t have free reusable objects, the serving requested is delayed until another
    object gets free from another client. *Figure 4* shows a flowchart to explain
    what happens when a client needs an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The object pool pattern](img/4666_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: ObjectPool flowchart'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pausing for a moment on the chart, we can see how important it is that each
    client always returns the object as soon as its use is no longer necessary: when
    the limit is reached, the pool cannot create new reusable objects and the client
    waits indefinitely, blocking all the executions. For this reason, we need to make
    sure that every client has this behavior. From a client''s perspective, the use
    of the pool changes its behavior just by adding this particular action of returning
    the used object. It also needs to be aware that sometimes the pool cannot return
    an object because none of them are available at that moment: then, it needs to
    handle this particular exception to the typical flow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another point to be aware of is that the object that''s just been used should
    be restored to a particular consistent state before being passed to another asking
    client in order to maintain clean management of the objects: the client doesn''t
    know that the object acquired has already been used by another client and it cannot
    receive the object in an unexpected state that can lead to unexpected behaviors.
    This can also produce a memory leak if the reusable objects reference other objects
    that keep being referenced by that after its release by the client. So, in most
    situations, the reusable object should be restored to a state as if it had just
    been created.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, if this pattern needs to be used in a multithreaded environment, it has
    to be implemented in a thread-safe way to avoid concurrent modifications to the
    pool's objects.
  prefs: []
  type: TYPE_NORMAL
- en: When the pool is first used, it's empty, and every time a client needs a reusable
    object, it is created from scratch. So, for newly created objects, there is a
    lag in their allocation. It could be a good idea, in some situations, if this
    fits your strategy, to allocate a number of objects as the pool is created to
    save time for future access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a quick overview of a simple code implementation of this pattern.
    The `ObjectPool` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We used two Sparse arrays to save the collection of objects and to prevent
    those objects from being collected when lent. We defined an initial capacity for
    the pool and a maximum one: this way, if there are too many request to manage,
    new objects can be created until the maximum capacity or all the requests are
    served. We delegated the creation of the object to the concrete class or to the
    direct implementation to let it have more flexibility. The two public methods
    are `ObjectPool.acquire()` and `ObjectPool.release()`: the clients can use them
    to ask for pre-allocated objects and give them back to the pool.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an `ObjectPool` interface inside Apache Commons with some useful implementations.
    That class uses a different name for methods used by the client: they are `ObjectPool.borrowObject()`
    and `ObjectPool.returnObject()`, and they add a special method, `ObjectPool.close()`
    to free the pool''s memory when done.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps not everyone knows this pattern, but it''s used a lot in everyday developing
    life: `AsyncTask` worker thread executions and `RecyclerView` recycled views are
    examples of the use of this pattern. This doesn''t mean we should use it in every
    situation. It should be used sparingly because of its pitfalls, but it can be
    really helpful in some situations.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When our code needs to allocate a lot of expansive instantiation objects, we
    could use `ObjectPool` to limit garbage collection and avoid memory churns. In
    every other situation, classic garbage collection is enough to handle our object's
    life cycle. If we decide to use this pattern, we need to use it carefully because
    we are responsible for releasing every object from the client and restoring the
    starting state for the reused object in order to avoid memory leaks. We also need
    to be sure to do it in a thread-safe way if in a multithreaded environment.
  prefs: []
  type: TYPE_NORMAL
- en: The FlyWeight pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many developers confuse the object pool pattern with the FlyWeight pattern,
    but they have different scopes: while the object pool''s aim is to reduce the
    impact of allocation and garbage collection in an environment with a lot of highly
    expensive objects, the FlyWeight pattern''s aim is to reduce the load into memory
    by saving the state shared by all of the objects. For this reason, we will consider
    two types of state for the object clients are asking for:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Internal state**: This is composed by fields that identify the object and
    are not shared with other objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External state**: This is the set of fields shared between all the exchanged
    objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, what the FlyWeight pattern does is reuse their internal state by creating
    just one instance of it for all of the objects, saving the cost of replicating
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flowchart of this pattern is shown in *Figure 5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The FlyWeight pattern](img/4666_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The FlyWeight pattern''s flowchart'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this pattern, there are three actors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FlyWeightObjects`: They can change the internal state and access the internal
    object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FlyWeightFactory`: This creates `FlyWeightObjects` when the client asks for
    them, managing their internal state. It can also be responsible for storing a
    pool of `FlyWeightObject` to lend to clients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Clients`: They ask for `FlyWeightObjects` and can change their intrinsic state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, there is a pool of `FlyWeightObjects`, but no borrowing this time. The
    memory related to the `FlyWeight` objects is freed by garbage collection when
    they are no longer referenced, as in the classic Java case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the code for this pattern. We need an interface to define methods
    for `FlyWeightObjects`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need at least one implementation of our interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The client this time is an object that uses the implementation of the interface
    as part of its status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, `Delivery` asks `Factory` for `Courier` and it joins the object
    state. But let''s see `Factory`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`Factory` holds a Sparse array of defined couriers. Note that no more than
    one instance for each type is created. Then every time a new `Delivery` is created,
    the `Factory` will give it the same `Courier` object. Hence, it will be shared
    and, in this particular case, every `Delivery` will be completed by the same `Courier`,
    as in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Android component leaks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following section, we will focus on particularly obnoxious memory leaks
    that we often don''t realize. When dealing with main components, memory leaks
    have an important impact on the overall performance of our applications: if we
    understand how to avoid them and we are very careful about these details, we will
    see a significant improvement in our app''s responsiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: Activities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Activities are the most commonly used components in an Android application and
    are the only ones with a user interface. There is a strong reference between the
    activity and every single contained view. This makes them particularly vulnerable
    to memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of different memory leaks related to activities, so let's deal
    with all of them, keeping in mind that we must avoid them all in order to have
    a fast environment for our applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'An activity is really expensive to keep in memory when no longer referenced.
    It references a lot of other objects that cannot be collected if the activity
    itself can''t. Furthermore, an activity can be destroyed and recreated many times
    during our application''s life cycle, for configuration changes or memory reclamation.
    If the activity is leaked, every instance of it may be stored in memory indefinitely,
    with a really expensive effect on memory. So, this is the worst mistake we can
    make in our code: never leak an activity. But how is it possible to leak an activity?
    You will be surprised how easy it is. Keep in mind that the system is destroying
    and creating activities for you when particular events occur, like a configuration
    change. Let''s go through some examples of common mistakes to better know how
    to avoid them, but before that, here''s a simple tip:'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is much easier to find a memory leak than to find the cause. But most of
    them are behind static classes, both static fields with activity dependencies
    and singletons. When you are searching for an activity leak, begin checking if
    the static fields have a reference to the activity itself. And, then, if this
    is not enough, find all the places you used the keyword `this` inside the activity
    code, because the instance can be used in different ways, maybe for a strong reference
    to an object with a longer lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a general rule to avoid activity leaks, sometimes, when we don''t need specific
    activity methods, we could use the application context instead of the activity
    itself by calling the `Context.getApplicationContext()` method: this uses an object
    that certainly won''t need to be collected before the application ends, just because
    it''s the application itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Static fields
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Static fields are really dangerous: they can reference and/or be referenced
    by activities and/or other objects causing the most of our memory problems. As
    we all know, the lifetime of a static object matches the application''s lifetime,
    meaning that it cannot be collected until the end. For example, if we declare
    a static `View` in our code, it will leak its activity as long as it''s not null,
    because every view holds the reference to its own activity. The following code
    shows a typical case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'When the `Activity.setContentView()` method is called, every `View` inside
    the layout XML file is instantiated using the `Activity` class as reference for
    `Context`. Look at its constructors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'It doesn''t matter how the `View` is instantiated: it needs to reference the
    `Activity` class, hence the memory leak if the `View` is declared as a `static`
    field. This is not related just to views, but it can happen with every object
    that references `Activity`. Furthermore, this can be extended to objects referenced
    by views: the background `Drawable` strong-references its `View`, which strong-references
    the `Activity`. This means that the following code has the same side-effect of
    the previous one, as the activity leak is still occurring, even if `View` is non-static
    this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Someone might think it's easier to fix this problem by setting the views to
    null when the activity life cycle is about to finish, for example, in `Activity.onStop()`
    or in the `Activity.onDestroy()` callbacks, but this could lead to `NullPointerException`
    if the instantiations at creation time are not properly handled, turning this
    solution into a dangerous one. Simply, avoid the use of static variables to avoid
    the memory leaks mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Non-static inner classes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Non-static inner classes are largely used in Android because they allow us
    to access outer classes fields without passing its reference directly. Then, many
    times Android developers add inner classes to save time, heedless of the effects
    on memory performance. Let''s create an inner class to explain what happens in
    this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: A simple `AsyncTask` is created and executed when the `Activity` is started.
    But the inner class needs to have access to the outer class for all of its lifetime,
    so memory leaks occur every time the `Activity` is destroyed, but the `AsyncTask`
    is still working. This happens not only when the `Activity.finish()` method is
    called, but even when `Activity` is destroyed forcibly by the system for configuration
    changes or memory needs and then it's created again. `AsyncTask` holds a reference
    to every `Activity`, making it not available for garbage collection when it's
    destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about what happens if the user rotates the device while the task is running:
    the whole instance of `Activity` needs to be available all the time until `AsyncTask`
    completes. Moreover, most of the time we want `AsyncTask` to put the result on
    the screen using the `AsyncTask.onPostExecute()` method. This could lead to crashes
    because the `Activity` is destroyed while the task is still working and views
    references may be null.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So what is the solution to this? If we set the inner class as a `static` one,
    we cannot access the outer one, so we need to provide the reference to that. In
    order to increase the separation between the two instances and let the garbage
    collector work properly with the `Activity`, let''s use a weaker reference to
    achieve cleaner memory management. The previous code is changed to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This way, the classes are separated and the `Activity` can be collected as soon
    as it's no longer used and the `AsyncTask` object won't find the `Activity` instance
    inside the `WeakReference` object and won't execute the `AsyncTask.onPostExecute`()
    method code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used `AsyncTask` for the example, but we could cancel it in the `Activity.onDestroy()`
    method, but it''s just an example of what can happen when using non-static inner
    classes. For example, the following code would result in the same mistake because
    the inner class is not static and holds a strong reference to `MainActivity`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As a general good practice, use weaker references than activities when you are
    dealing with threads, even if the thread is not an inner class.
  prefs: []
  type: TYPE_NORMAL
- en: Singletons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we all know, a `singleton` is an object that can be instantiated once for
    the entire lifetime of the application. This is really helpful to avoid duplications
    of data, to share data with multiple objects of our code, and to have global access
    to it. However, we need to be careful of what is referenced by `singleton` because
    of its lifetime. If we use an `Activity` reference in a singleton and we don't
    free it, it will be leaked until the application ends. This can be applied to
    any other type of objects, but as we know, the `Activity` leak is particularly
    frightening and we want to focus on that at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following code, which represents a `Singleton` class
    with an interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, let''s look at the `Activity` code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the `Singleton` object will have `MainActivity` as a reference
    until it''s destroyed and, then, until the application is destroyed. In this situation,
    it is really important to remove the reference when the `MainActivity` needs to
    be freed. Then, the previous `MainActivity` code can be changed into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, we could use the same solution adopted in the previous example:
    if we use a `WeakReference` for the callback inside `singleton`, the `Activity`
    can be collected when needed. This solution would change the code into this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Anonymous inner classes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The specialization of classes or interfaces in a class suffers from the same
    problem described for the non-static inner classes and singleton cases: anonymous
    inner classes need the outer class to be stored and, then, they leak it. Let''s
    see the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This is the same code as for the previous Singleton example, but the `Activity`
    doesn't implement the `Callback` interface that is, instead, instantiated as an
    anonymous inner class. As mentioned, this is still a problem, and both the solutions
    approached earlier are still valid.
  prefs: []
  type: TYPE_NORMAL
- en: Handlers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A leak related to all of the ones discussed so far is the `Handler` leak. This
    is insidious because it''s not so obvious. Fortunately, Lint checks for it and
    warns us. So, inspect your code to find it. A `Handler` object can execute delayed
    code using the `Handler.postDelayed()` method and this is the problem. Take a
    look at the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Handler` object posts its `Runnable` interface to `LooperThread` until
    execution. But we know that an anonymous inner class has a reference to the outer
    class that is the `Activity` in our example, hence the activity leak. But `LooperThread`
    has a queue of messages to execute `Runnable`. Then, even if our handler doesn''t
    post a delayed message, but it''s used just because you need to change the UI
    (and you use the `Handler` object to execute those changes on the main thread,
    as we know this is the only thread that can do it), memory leaks can occur if
    the queue is large. So, as with anonymous inner classes, let''s export that class,
    setting it as `static`, and let''s pass the reference to the `TextView` because,
    as it''s `static`, it cannot access it anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Have we got rid of the leak? Unfortunately, no. `TextView` still has a reference
    to the container `Activity` because it''s a view and it''s still referenced. So,
    let''s apply the second solution we found for inner classes, using a `WeakReference`
    to store the `TextView`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This way, the activity can be collected properly when needed and no leaks occur
    any more. But there is one more point of improvement for this code: it may be
    helpful to remove every message from the queue. This way, we are sure that the
    queue is cleaned, the `Activity` can be destroyed, and the code in the `Runnable`
    object won''t be executed when the `Activity` is no longer available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Services are addressed in depth in [Chapter 5](ch05.html "Chapter 5. Multithreading"),
    *Multithreading*, but we want to see how services can impact memory performance
    during the normal application life cycle. The system stores the active process
    using a cache with the **Least Recently Used** (**LRU**) pattern, meaning that
    it can force the closure of previously used processes, keeping the latest ones.
    Then, every time we keep a service active that is no longer used, we not only
    create a memory leak with a service, but we also prevent the system from cleaning
    up the stack to insert new processes. So, it's really important to pay appropriate
    attention to the closure and the release of a service that has just finished performing
    work in the background.
  prefs: []
  type: TYPE_NORMAL
- en: As we will see in the next chapter, a service can be stopped with the `Service.stopSelf()`,
    if internally called, or with `Context.stopService()`, if externally. This must
    be done every time it's not working anymore because the `Service` object doesn't
    finish. But in order to improve the memory and process management of our app,
    we should use `IntentService` instead of a simple `Service` as much as possible,
    because this kind of service finishes automatically when the background work is
    completed.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use `IntentService` every time you can because of the automatic finalization
    and because this way you don't risk creating memory leaks with services. This
    is one of the worst memory mistakes we can make. So, if you cannot use `IntentService`,
    make sure that the `Service` is finished as soon as it completes its task.
  prefs: []
  type: TYPE_NORMAL
- en: Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some applications use a special technique to separate memory loads through
    different processes. As we will see in [Chapter 5](ch05.html "Chapter 5. Multithreading"),
    *Multithreading*, every component in Android is executed by default in the main
    process, but they can be executed in separate ones, by simply defining the process
    name in the manifest file for every component you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This is good for profiling the code because you can analyze a single process
    without affecting the others. Moreover, it simplifies the Android system process
    management. But we must be careful to properly manage memory, otherwise we risk
    having the opposite effect and, instead of decreasing the memory allocation, we
    increase it. So, some simple tips to create a multiprocess application are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Common implementations are duplicated in every process, so try to reduce them.
    The separation between processes should be clean and common objects should be
    cut down as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The UI should be handled by just one process, because the memory allocated for
    it depends on a lot of factors, such as bitmaps and resource allocations. Anyway,
    the application can show one activity at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relationships between processes are really important, because a process
    cannot be deleted by the system if it is dependent on another one. This means
    we need to be aware of using components that can access more processes, because
    in this case, the advantages in memory performance are nullified. So, be careful
    when using components, just like `ContentProvider` and `Service`, accessed by
    multiple processes. Profile your code to analyze implications in situations such
    as this in order to improve the architecture of your solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What do we do if our application is in a low memory situation? And what if our
    application needs to allocate too much memory? Let's have a look at what the platform
    provides and if it's really helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different devices mean different amounts of RAM to allocate memory. Then, our
    app will have to be responsive to this particular requirement. Android provides
    a particular way to ask for a large heap for our application. It can be done by
    adding the attribute to the `application` node in the manifest file, as in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'But this large quantity of memory is asked for every single process created
    by the application. This is just a request to the system and we are not sure our
    processes will have a larger heap than in the normal case. And remember, this
    is not intended to be used if we are unable to have a free memory management in
    our application and/or you are facing `OutOfMemoryError`. If you are facing such
    an error, then profile your code, catch any memory anomalies you can, and reduce
    memory leaks. Just a couple of applications should be able to ask for a large
    heap: those with an extreme justified need of memory. In general, they are applications
    that deal with high-level photos, videos, and multimedia editing. Then this trick
    may avoid `OutOfMemoryError`, but it may also produce an effect related to garbage
    collection timings: the higher the available heap is, the higher the collection
    limits are, the more time the collector needs to collect. Hence, this increased
    duration in collection may affect our 16 ms target, resulting in UI lags.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Never use the `largeHeap` attribute inside the Android manifest file to avoid
    `OutOfMemoryError`: it''s not a solution, not a trick. On the contrary, it may
    lead to UX problems and it may affect the overall device performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a helpful class called `ActivityManager` that provides methods to
    ask for info about memory consumption and availability. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getMemoryClass`: This returns the megabytes that are provided to the application.
    This can be used to estimate the amount of memory we will use or the quality of
    the images used in the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getLargeMemoryClass`: This is the same as the `getMemoryClass()` method, but
    this is for large heap requested cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getMemoryInfo`: This returns a `MemoryInfo` object containing useful information
    about memory system-related states:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`availMem`: Available system memory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lowMemory`: A boolean value that shows if the system is in low memory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold`: The threshold of memory above which the system is in low memory
    and can start the removing processes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getMyMemoryState`: This returns `RunningAppProcessInfo` containing useful
    information about the calling process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lastTrimLevel`: This is the last trim level for the process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`importance`: The importance of the process. As we will see in [Chapter 5](ch05.html
    "Chapter 5. Multithreading"), *Multithreading*, every process has its own priority
    and the system will decide to remove it based on its level.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isLowRamDevice`: This returns whether the device needs to be considered as
    a low memory device. This can be useful to enable or disable features depending
    on the memory we need.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'This particular method has been added to the platform from Android KitKat (API
    Level 19), but there is a compatibility class that does the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: As the last one, let's talk about the `System.gc()` method that forces the request
    to trigger the garbage collector. It can be used everywhere, but it's not guaranteed
    if and when the garbage collector will be triggered. Furthermore, we should prefer
    to have a consistent strategy to follow to manage memory during our application's
    life cycle and profile our code to find memory leaks and churns.
  prefs: []
  type: TYPE_NORMAL
- en: Main components and memory management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Among the four main components that Android provides, the `BroadcastReceivers`
    are the only ones that don''t need a specific memory management strategy: their
    life cycle is related to the only `BroadcastReceiver.onReceive()` method, and
    they are destroyed just after the execution of it. Obviously, this is not valid
    for the other three main components, as they live until we destroy them or the
    system does, when it needs memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, they all implement the `ComponentCallback` interface. We are
    interested in one method in particular: the `ComponentCallback.onLowMemory()`
    method. Its implementation is executed every time the system is running on low
    memory and before it starts to kill processes. So this is a good chance to release
    some of the memory of our application. We are not talking about memory leaks,
    but other kinds of memory holding, such as heap cached objects. Then, override
    the method to free held objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this `ComponentCallback.onLowMemory()` method is called after
    the system has already started killing other processes. This is not good because
    an application is much more expensive to recreate from scratch than to resume
    from the background. This is why, during the development of the Android platform,
    the callback described above has been improved by defining a sub interface for
    the `ComponentCallback` called `ComponentCallback2`. It introduces a more specific
    method as well as inheriting the `ComponentCallback.onLowMemory()` method. It's
    available from Android Ice Cream Sandwich (API Level 14) onwards. This means that
    the main components from Android 14 implement this one instead of the `ComponentCallback`
    interface, so the `ComponentCallback` method isn't available on earlier versions.
  prefs: []
  type: TYPE_NORMAL
- en: The method we are talking about is the `ComponentCallback2.onTrimMemory()` method.
    The idea behind it is the same as for the `ComponentCallback.onLowMemory()` method,
    but here the system provides us the level of criticality of memory consumption
    in the system. There are two different states our application can be, related
    to its visibility, and every state can receive different level of memory. As mentioned
    before, all the processes in the system are managed using a LRU policy, defining
    a list from current processes at the top to older processes at the bottom. The
    one at the bottom is the first to be deleted to recover memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s see the visibilities for the application and their LRU positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visible**: The app is currently running and it''s on top of the LRU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invisible**: The app is no longer visible and it starts to fall down the
    list until it''s destroyed after reaching the tail, or it''s moved to the top
    again when it becomes visible again'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `ComponentCallback.onTrimMemory()` method passes an integer value as a
    parameter. Depending on this parameter we can take different actions to prevent
    the process reaching the bottom and being destroyed. In this case, the application
    needs to be initialized again: this is more expensive than retrieving data to
    recover a previous state of the cache.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The constants used as parameters in this methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TRIM_MEMORY_RUNNING_MODERATE`: The application is visible and the system is
    starting to get on low memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRIM_MEMORY_RUNNING_LOW`: The application is visible and the memory device
    is getting lower.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRIM_MEMORY_RUNNING_CRITICAL`: The application is visible and the memory device
    is critical, and other processes may be destroyed in order to free memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRIM_MEMORY_UI_HIDDEN`: The application is invisible. This is just a callback
    to notify that the application is no longer visible and you should free some memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRIM_MEMORY_BACKGROUND`: The application is invisible and it has started the
    descent in the LRU list and the device is running low on memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRIM_MEMORY_MODERATE`: The application is invisible and it has reached the
    middle of the LRU list and the device is running low on memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRIM_MEMORY_COMPLETE`: The application is invisible and it has reached the
    bottom of the LRU list and the device is running low on memory, so the application
    is about to be killed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the system starts to kill processes, it decides which one to kill by analyzing
    the memory consumption. This means that the less memory our app is consuming,
    the less likely it is to be killed, and the faster resume it will have.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the application is well structured memory-wise, a good practice to free
    memory when such events are triggered may be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: If you free objects from different caches or levels, removing the breaks from
    the `switch` statement, every case is executed again to free memory in every more
    critical state.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the main components, this interface is implemented by the `Application`
    and `Fragment` classes as well. This way we can free memory inside single fragments
    too, using the `onTrimMemory()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowing what a memory leak and a memory churn are and what strategies we can
    pursue to avoid them, we now need to know how we can find them and how we can
    profile our code from a memory perspective.
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned several times in this chapter, we must always keep an eye
    on the amount of heap memory used by our application processes, trying to keep
    it as low as possible and to free resources as much as possible while checking
    the garbage collector's behavior. Our application needs to be able to stay together
    with other applications on devices with the most varied amounts of RAM. Therefore,
    keeping that in mind, we will focus on helpful tools able to analyze the memory
    usages and we will know how to read common logs related to garbage collection.
  prefs: []
  type: TYPE_NORMAL
- en: LogCat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest tool to start with is surely LogCat, which is used to print messages
    that inform us about memory trends and garbage collection events. Every message
    related to memory in LogCat has the same format depending on the device runtime.
    That's why we will check both the Android runtimes, starting with Dalvik and following
    with ART. Developers, in general, do not spend enough time analyzing these logs.
    They are very important if we want to understand if the behavior of our application
    is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Dalvik
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Dalvik memory log print has the following format in the LogCat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand the meaning of every element in the log:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GcReason`: This is the reason why the garbage collector has been triggered.
    All of the application threads are blocked waiting for the conclusion of collection.
    Possible values are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GC_CONCURRENT`: It follows the GC event when the heap needs to be cleared.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GC_FOR_MALLOC`: It follows the request of allocation of new memory, but there
    is not enough space to do it.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GC_HPROF_DUMP_HEAP`: It follows a debug request to profile the heap. We will
    see what this means in the following pages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GC_EXPLICIT`: It follows a forced explicit request of `System.gc()` that,
    as we mentioned, should be avoided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GC_EXTERNAL_ALLOC`: It follows a request for external memory. This can happen
    only on devices lower or equal to Android Gingerbread (API Level 10), because
    in those devices, memory has different entries, but for later devices the memory
    is handled in the heap as a whole.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmountFreed`: This is the amount of memory the garbage collector was able
    to free.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HeapStats`: This is referring to the internal heap and it''s composed of the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of the free heap over the total
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of allocated heap
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of total heap
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ExternalMemoryStats`: This is referring to the external memory for devices
    with Android Gingerbread (Api Level 10) or lower. It contains the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of allocated external memory
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of total external memory
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PauseTime`: This is the duration of the pause for the garbage collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example of Dalvik log to show how it could be in the LogCat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: ART
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The ART memory log has a quite different format, but it''s still readable.
    However, ART has different behavior from the Dalvik runtime: not every garbage
    collector event is logged into LogCat. ART logs just force events and events with
    garbage collector pause longer than 5 ms or durations longer than 100 ms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is its format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the elements in the log are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GcReason`: This is the reason why the garbage collector has been triggered.
    Possible values are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Concurrent`: It follows a concurrent GC event. This kind of event is executed
    in a different thread from the allocating one, so this one doesn''t force the
    other application threads to stop, including the UI thread.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Alloc`: It follows the request for the allocation of new memory, but there
    is not enough space to do it. This time, all the application threads are blocked
    until the garbage collection ends.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Explicit`: It follows a forced explicit request of `System.gc()` that should
    be avoided for ART as well as for Dalvik.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NativeAlloc`: It follows the request for memory by native allocations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CollectorTransition`: It follows the garbage collector switch on low memory
    devices.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HomogenousSpaceCompact`: It follows the need of the system to reduce memory
    usage and to defragment the heap.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DisableMovingGc`: It follows the collection block after a call to a particular
    internal method, called `GetPrimitiveArrayCritical`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HeapTrim`: It follows the collection block because a heap trim isn''t finished.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GcName`: ART uses different garbage collectors to free memory and they have
    different behaviors, but we have no choice for that and this information is not
    very useful for our analysis. Anyway, possible values for the name are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Concurrent mark sweep (CMS)`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Concurrent partial mark sweep`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Concurrent sticky mark sweep`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Marksweep + semispace`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ObjectFreed`: The number of freed objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SizeFreed`: The total size of freed objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LargeObjectFreed`: The number of freed objects from the large space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LargeObjectSizeFreed`: The total size of freed objects from the large space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HeapStats`: This is like the Dalvik one. It contains the percentage of free
    heap space, the size of allocated heap, and the total heap size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PauseTimes`: This is the duration of the pause for the garbage collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see an example of an ART log as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The ActivityManager API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already talked about this class before, but this time we want to show
    other methods that can be helpful while profiling the application from the memory
    point of view. There are two methods that help us find memory-related problems
    when debugging and they can only be used if the application is debuggable. We
    are talking about the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setWatchHeapLimit`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clearWatchHeapLimit`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first one, in particular, allows us to set an alarm on the heap memory:
    when the set amount of heap has been reached, the device will automatically pick
    a heap dump and we can analyze the result to understand if a memory leak occurred.
    The second one has the aim of removing the set limit. Furthermore, this class
    provides an action to be handled by an `Activity` or a `BroadcastReceiver` to
    notify us that the limit has been reached and a heap dump has been picked. This
    action is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, these methods are available only from Android Marshmallow (API
    Level 23), but this way we can keep testing while the system is profiling the
    memory for later analysis.
  prefs: []
  type: TYPE_NORMAL
- en: StrictMode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another really helpful API provided by the platform is `StrictMode`. This class
    is used to find memory and network problems. We will deal with just the memory
    part here, while in [Chapter 6](ch06.html "Chapter 6. Networking"), *Networking*,
    we will deal with the network counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: 'If enabled, it operates in the background and notifies us that there is an
    issue and when it happens, depending on the policy that we choose. Then, there
    are two things to define when using this: what to track and how. For this, we
    can use the `StrictMode.VmPolicy` class and the `StrictMode.VmPolicy.Build` class
    this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what we can observe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`detectActivityLeaks`: It detects activity leaks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detectLeakedClosableObjects`: It detects if a `Closable` object is finalized,
    but not closed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detectLeakedRegistrationObjects`: It detects if `ServiceConnection` or `BroadcastReceiver`
    is leaked when `Context` is being destroyed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detectSqlLiteObjects`: It detects if an SQLite object is finalized, but not
    closed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detectAll`: It detects every suspicious behavior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They can be used together to detect multiple events. And now, let''s see how
    it can notify the developer:'
  prefs: []
  type: TYPE_NORMAL
- en: '`penaltyDeath`: When a detection happens, the process is killed and the app
    crashes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`penaltyDropBox`: When detected, the relative logs are sent to `DropBoxManager`
    that collects them for debugging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`penaltyLog`: When a detection occurs, it''s logged'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It''s really helpful to understand which class isn''t respecting the limit
    by specifying its name and the occurrences. The following is an example of a log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enable `StrictMode` in debugging and testing environments to detect any memory
    problems and, above all, as we discussed previously in this chapter, activity
    leaks. Remember to disable it for release builds because it can be used for different
    detection in future Android versions and because, even if it's silent, it's active
    in the background, consuming resources that we may need to reach our performance
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: Dumpsys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Dumpsys tool is in every Android device and it lets us get an impressive
    amount of information about every service inside the device. It can be used in
    a terminal by calling the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The service is optional, but if you don''t specify that it is the service you
    are interested in, the result of all of them will be printed, and it can be a
    little confusing. The service''s availability depends on the particular Android
    version installed on the device. Then, call the following for the complete list
    of available services on your device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'For every one of them, you can see the possible argument you can add, by simply
    calling the same as before and adding the `–h` argument at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: In the following pages, we will show two services of `dumpsys` that are particularly
    useful to profile our code from a memory point of view.
  prefs: []
  type: TYPE_NORMAL
- en: Meminfo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Meminfo tool shows important information about memory usage on the device.
    The command used to invoke it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The **PSS** is Linux's **Proportional Set Size** metric. It refers to the total
    amount of memory used by the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can go further by asking for detailed information about a particular process
    using its pid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will see something like the following printed on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'It contains the memory usage of our application in the foreground. The first
    two columns of the table refer to allocated memory that we should monitor: unexpected
    values there could mean memory leaks.'
  prefs: []
  type: TYPE_NORMAL
- en: ProcStats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Android KitKat (API Level 19) introduced the ProcStats tool, which is able to
    provide important information about processes and their memory. It can profile
    the use of all of the processes related to the application, tracking background
    or foreground processes, their memory usage, and running times.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command to use to see general statistics of the entire system is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this is a list of processes sorted by running times. Let''s see
    an example of that to understand how it can be read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Every process shown in the list has the memory status over the last three hours
    in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'While we already saw what PSS is, **USS** stands for **Unit Set Size**, and
    it''s private memory. So, let''s see the meaning of those values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`percent:` It is the time percentage over the three hours of execution of the
    process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minPss`: Minimum total memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`avgPss`: Average total memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxPss`: Maximum total memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minUss`: Minimum private memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`avgUss`: Average private memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxUss`: Maximum private memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we want to see detailed information about a particular application, we
    can use the following, that is, the same as the previous one, but this time we
    added the package of the application to analyze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The printed result for this looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we can analyze the memory usage in different system memory-related
    states. The above printout means that the device status changed from normal to
    moderate status, or to low memory, or critical. Our application freed resources
    and the total amount of memory dropped because of that. We also know the time
    spent in those particular states, based on what is inside **Run Time Stats** inside
    **Summary**.
  prefs: []
  type: TYPE_NORMAL
- en: This is really useful to understand if the policy you used for when an `onTrimMemory()`
    event is triggered by the system is correct or if it can be improved by freeing
    more objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ProcStats tool is also available directly inside the device: open **Developer
    settings** and then **Process Stats**. You will see something like what is shown
    in *Figure 6*, where the left screen shows the list of background processes and
    their percentage over time, while the right screen shows the details of a process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ProcStats](img/4666_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: ProcStats on device'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the menu, it is possible to change the duration and the type of switching
    of the following processes:'
  prefs: []
  type: TYPE_NORMAL
- en: Background processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foreground processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cached processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The progress bar in the **Process Stats** screen can change its color depending
    on the memory states:'
  prefs: []
  type: TYPE_NORMAL
- en: Green, if the memory state is normal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yellow, if the memory state is moderate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Red, if the memory state is low or critical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the research to improve the performance of an Android application, memory
    is central and takes a leading role in the perception of our application by the
    users, despite being the most ignored aspect by developers during the development
    process. Every developer should spend some time checking the memory management
    of the application they are working on: there are many chances for memory leaks.
    That is why we focused on how Android garbage collection works, what the main
    causes of memory leaks are, and what a memory churn is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We defined a lot of best practices to help maintain good memory management,
    introducing helpful design patterns and analyzing the best choices while developing
    things taken for granted that can actually affect memory and performance. Then,
    we looked at the main causes for the worst leaks in Android: those related to
    main components such as activities and services. As a conclusion for the practices,
    we introduced APIs both to use and not to use, then, other APIs able to define
    a strategy for events related to the system and external to the application.'
  prefs: []
  type: TYPE_NORMAL
- en: The aim of the last part of this chapter was to make the developer able to read
    memory logs and let them identify the right tool to search for memory anomalies
    during the debug step and collect data analysis to profile the application. This
    way they can easily find leaks, then search for the triggering code, and finally
    apply a fix, following the defined best practices, or improving the memory management
    of their application.
  prefs: []
  type: TYPE_NORMAL
