- en: Chapter 9. Collecting and Storing Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章 数据收集与存储
- en: Onwards we go! In the previous chapter, we introduced a couple of popular external
    databases that you could use and decided to develop a fully functional backend
    using Google's App Engine (GAE). We managed to create a new project on GAE and
    use the `PersistenceManager` to build out an extremely useful wrapper class that
    illustrated some of the concepts central to our JDO database. This wrapper class
    will soon be extremely handy to have around as we start inserting real data and
    subsequently query that data using our Android application.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续前进！在上一章中，我们介绍了一些你可以使用的外部数据库，并决定使用谷歌的应用引擎（GAE）开发一个功能齐全的后端。我们在GAE上成功创建了一个新项目，并使用`PersistenceManager`构建了一个非常实用的包装类，该包装类展示了我们JDO数据库中的一些核心概念。当我们开始插入实际数据，并随后使用我们的Android应用程序查询这些数据时，这个包装类将非常方便。
- en: So here we are — the next step! For most people trying to build out a data-centric
    application, actually getting that data will be extremely difficult and will typically
    require a lot of time and money. However, there are many tools and methods at
    our disposal which can help us use existing data to fill up our databases. In
    this next chapter, we'll take a look at some of those methods, and will finish
    by inserting our newly acquired data into our JDO database.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来就是这里——下一步！对于大多数试图构建以数据为中心的应用程序的人来说，实际获取这些数据将极其困难，通常需要大量的时间和金钱。然而，我们现在有很多工具和方法可以帮助我们利用现有数据来填充我们的数据库。在接下来的章节中，我们将研究其中一些方法，并最终将我们新获取的数据插入到JDO数据库中。
- en: Methods for collecting data
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集方法
- en: 'To begin, let''s briefly go over two different ways in which you can collect
    data:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们简要回顾一下你可以收集数据的两种不同方式：
- en: Through an Application Programming Interface (API)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过应用程序编程接口（API）
- en: Through web scraping
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过网络抓取
- en: The first and simplest way is through using an API. For those who have never
    used an API before, think of this as a *web library* created by some third-party
    company, which typically allows you to call a handful of functions (almost always
    executed as HTTP requests), which then give you access to a subset of their data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种也是最简单的方式是使用API。对于那些以前从未使用过API的人，可以将这看作是由第三方公司创建的*网络图书馆*，通常允许你调用一些函数（几乎总是以HTTP请求的形式执行），从而访问他们数据的一个子集。
- en: For instance, a common API is the Facebook Graph API which, when authenticated,
    allows you to query for a user's profile information or an event's details, and
    so on. Essentially, through the API, I can access the same data about a person
    or event that I would see on Facebook's website, just through a different channel.
    This is what I mean by the company *exposing* a subset of their data. Another
    example might be with Yelp, whose API allows you to query for restaurants and
    venues when passed a set of parameters (that is, location). Here, even though
    I'm not actually on Yelp's web page, I can still access their data through their
    API.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个常见的API是Facebook Graph API，当通过验证后，它允许你查询用户的个人资料信息或事件的详情等。本质上，通过API，我可以访问到在Facebook网站上能看到的人或事件的相同数据，只是通过不同的渠道。这就是我所说的公司*公开*其数据的一个子集。另一个例子可能是Yelp，它的API允许你通过传递一组参数（即位置）来查询餐馆和场所。在这里，即使我实际上没有在Yelp的网页上，我仍然可以通过他们的API访问到他们的数据。
- en: Having an API available to collect your data is extremely useful because of
    how the data is already there and ready for you to use; depending on the credibility
    of the company, oftentimes the data will already be cleaned and well formatted.
    This saves you from having to find the data on your own and subsequently clean
    the data on your own. The catch, however, is that oftentimes companies will not
    allow you to store their data for proprietary reasons, and so depending on what
    your application does, you may need to keep this legal issue in mind.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个API来收集你的数据非常有用，因为数据已经存在并准备好供你使用；根据公司的信誉，通常数据已经被清理并格式化得很好。这让你不必自己寻找数据，然后自己清理数据。然而，问题是，通常公司出于版权原因不允许你存储他们的数据，所以根据你的应用程序的用途，你可能需要考虑这个法律问题。
- en: So what exactly happens when no API is available for you to use? Well, then
    you'll have to resort to getting that data on your own, and one great way to do
    that is through **web scraping**. In the next section, I'll devote a great deal
    of time to explaining what the art of web scraping is and how you go about doing
    it. For now, let's end this short section with a discussion on the two popular
    formats in which data is often returned by APIs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果没有可用的API供你使用，会发生什么呢？这时，你可能需要自己获取数据，而进行**网络爬虫**是完成这项任务的一种好方法。在下一节中，我会花大量时间解释网络爬虫的艺术以及如何进行爬虫操作。现在，让我们以讨论API经常返回数据的两种流行格式来结束这个简短的部分。
- en: The first is called Extensible Markup Language (XML) and is a human-readable
    and machine-readable data format that takes the form of a tree and looks very
    similar to HTML actually. A simple example of what this tree structure looks like
    is say you call the Facebook Graph API and it returns a list of your friends.
    The root of the tree might have the tag`<friends>`, and underneath it may have
    a series of leaves with the tag`<friend>`. Then, each`<friend>` node might branch
    off into several descriptor tags such as`<name>, <age>`, and so on. In fact, in
    the examples later on, I'll actually use XML as the data format of choice because
    of how it's human readable, so you'll get to see real examples of what this looks
    like.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种是可扩展标记语言（XML），它是一种可读性强、以树形结构呈现的数据格式，实际上与HTML非常相似。举个例子，如果你调用Facebook图形API，它会返回你的好友列表。这棵树的根可能有一个标签`<friends>`，下面可能有一系列的叶子标签`<friend>`。然后，每个`<friend>`节点可能会分支出几个描述符标签，如`<name>,
    <age>`等等。实际上，在后面的例子中，我会使用XML作为首选的数据格式，因为它易于阅读，这样你可以看到这种格式的真实例子。
- en: The next is called **JavaScript Object Notation (JSON)** and it is a much more
    lightweight data structure than XML. JSON is still machine readable but is less
    friendly for human readability. The trade-off though is that parsing JSON tends
    to be more efficient, and so really the decision between which to use just depends
    on how important human readability is relative to performance. The general structure
    of JSON resembles that of a map instead of a tree. Using the same preceding example,
    instead of being returned a tree structure with`<friends>` as the root node, we
    might have `friends` as a key with value equal to a JSON array. The JSON array
    would then have a list of `friend` keys, each of which has a value equal to a
    JSON object. Finally, the JSON object would have keys equal to `name, age`, and
    so on. In other words, you can think of JSON structures as series of e mbedded
    maps, where many times keys will point to a sub-map, which then has its own keys,
    an so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下一种是**JavaScript对象表示法（JSON）**，它比XML更轻量级。JSON仍然可以被机器读取，但不如XML适合人类阅读。然而，其优点是解析JSON通常更高效，因此选择使用哪种格式实际上取决于人类可读性相对于性能的重要性。JSON的一般结构类似于映射而非树形结构。使用前面的例子，而不是返回以`<friends>`为根节点的树形结构，我们可能会得到一个以`friends`为键，值为JSON数组的结构。该JSON数组将包含一系列的`friend`键，每个键的值都是一个JSON对象。最后，JSON对象将包含等于`name,
    age`等键。换句话说，你可以将JSON结构看作是一系列嵌套的映射，很多时候键将指向一个子映射，该映射又有自己的键，依此类推。
- en: So often when using third-party APIs, you'll need to be aware of which data
    format they choose to return their data in, and parse the results accordingly.
    Furthermore, even when you're implementing web scrapers and finding yourself having
    to build your own API, it often helps to pick one of the two data formats and
    stick with it. This will make your life a lot simpler when it comes to calling
    your own API from external applications and then parsing the returned result.
    Now, moving on to web scraping.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用第三方API时，通常需要知道它们选择以哪种数据格式返回数据，并相应地解析结果。此外，即使在你实现网络爬虫并需要构建自己的API时，通常也最好选择两种数据格式之一并坚持使用。这样，在从外部应用程序调用你的API并解析返回结果时，你的生活会简单得多。现在，让我们来谈谈网络爬虫。
- en: A primer on web scraping
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络爬虫入门
- en: 'Web scraping is the art of structuring web HTML and methodically parsing data
    from it. The idea is that HTML should be (to some extent) inherently well structured,
    as every open tag (that is,`<font>)` should be followed by a close tag (that is,`</font>)`.
    In this way, HTML if structured correctly, can be viewed as a tree structure very
    much like XML often is. Scraping a website can be achieved in any number of ways,
    which typically vary with the complexity of the underlying HTML source code, but
    at a high level, it involves three steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取是将网页HTML结构化，并系统地从中解析数据的艺术。这个想法是HTML应该在一定程度上固有地具有良好的结构，因为每个开放标签（即`<font>`）都应该有一个关闭标签（即`</font>`）相对应。这样，如果HTML结构正确，它可以被视为一个树状结构，非常类似于XML。抓取一个网站可以通过多种方式实现，这通常与底层HTML源代码的复杂性有关，但在高层次上，它涉及三个步骤：
- en: Obtain the desired URL, establish a connection to the URL, and retrieve its
    source code.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取所需的URL，建立与URL的连接，并获取其源代码。
- en: Structure and clean the underlying source code so that it becomes a valid XML
    document.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组织和清理底层源代码，使其成为一个有效的XML文档。
- en: Run a tree-navigating language like XPath (or XQuery and XSLT), and/or use regular
    expressions (REGEX) to parse out desired nodes.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行像XPath（或XQuery和XSLT）这样的树遍历语言，以及/或使用正则表达式（REGEX）来解析所需的节点。
- en: The first step is relatively self-explanatory, but I will note one thing. Often
    you'll find yourself needing to scrape some sort of dynamic web page, meaning
    that the URL is not going to be static and may change depending on the date, some
    set of criteria, and so on. Let's walk through two examples of what I mean here.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步相对容易理解，但我需要指出一点。通常你会发现需要抓取某种动态网页，这意味着URL不是静态的，可能会根据日期、一组标准等而变化。让我们通过两个例子来解释我的意思。
- en: 'The first involves stocks. Let''s say you''re trying to write a web scraper
    that can scrape for the current price of a given stock, say from Yahoo! Finance.
    Well, first off, what does the URL look like? Checking really quickly for Google''s
    (ticker GOOG) current price, we see that the URL of the corresponding web page
    is:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个涉及股票。假设你正在尝试编写一个可以抓取给定股票当前价格的网页抓取器，比如从Yahoo! Finance获取。那么，首先，URL是什么样子的？快速检查谷歌（股票代码为GOOG）的当前价格，我们看到相应网页的URL是：
- en: '[http://finance.yahoo.com/q?s=GOOG](http://finance.yahoo.com/q?s=GOOG)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://finance.yahoo.com/q?s=GOOG](http://finance.yahoo.com/q?s=GOOG)'
- en: 'It''s a pretty simple URL and we''ll quickly notice that the ticker of the
    stock gets passed as a parameter to the URL. In this case, the parameter has key
    `s` and value equal to the ticker. Now it''s pretty easy to see how we can quickly
    construct a dynamic URL to solve our problem-all we would have to do is write
    a simple method as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的URL，我们会很快注意到股票代码作为参数传递给URL。在这种情况下，参数的键为`s`，值等于股票代码。现在我们可以很容易地看出如何快速构建一个动态URL来解决问题——我们只需要编写以下简单的函数：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Neat, right? Now let''s say we don''t just want the current stock price, but
    we want to pull all historical prices between two dates. Well, first let''s take
    a look at what a sample URL would be, again for Google''s stock:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 很整洁，对吧？现在假设我们不仅仅想要当前的股票价格，我们还想要获取两个日期之间的所有历史价格。首先，让我们看看一个示例URL是什么样的，再次以谷歌股票为例：
- en: '[http://finance.yahoo.com/q/hp?s=GOOG&a=07&b=19&c=2004&d=02&e=14&f=2012](http://finance.yahoo.com/q/hp?s=GOOG&a=07&b=19&c=2004&d=02&e=14&f=2012)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://finance.yahoo.com/q/hp?s=GOOG&a=07&b=19&c=2004&d=02&e=14&f=2012](http://finance.yahoo.com/q/hp?s=GOOG&a=07&b=19&c=2004&d=02&e=14&f=2012)'
- en: So what do we notice here? We notice that the ticker is still being passed as
    a parameter with key `s`, but in addition to that we notice what looks like two
    distinct dates being passed with various keys. The dates look like 07/19/2004,
    most likely the start date, and 02/14/2012, what appears to be the end date, and
    they seem to have key values `a` through `f`. In this case, the key values aren't
    the most intuitive, and oftentimes you'll see key values of `day` or `d` and `month`
    or `m` instead. However, the idea is simply that with this URL, not only can you
    dynamically adjust what the ticker is but depending on what range of dates your
    user is looking for, you can adjust those as well. By keeping this idea in mind,
    you'll slowly learn how to better decipher various URLs and learn how to make
    them extremely dynamic and suitable for your scraping needs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们在这里注意到了什么？我们注意到股票代码仍然作为参数传递，键为`s`，除此之外我们还注意到似乎有两个不同的日期被传递，带有各种键。日期看起来像07/19/2004，很可能是开始日期，以及02/14/2012，看起来是结束日期，它们似乎有键值`a`到`f`。在这种情况下，键值并不是最直观的，而且很多时候你会看到键值为`day`或`d`以及`month`或`m`。然而，这个想法仅仅是通过这个URL，你不仅可以动态调整股票代码，还可以根据用户需要查看的日期范围来调整这些日期。牢记这个想法，你会逐渐学会如何更好地解读各种URL，并学会如何使它们极具动态性，适合你的抓取需求。
- en: Now, some websites make their requests through POST requests. The difference
    is that in POST requests, the parameters are embedded within the request (as opposed
    to being embedded within the URL). This way, potentially private data is not visibly
    displayed in the URL (though this is just one use case for POST requests). So
    what do we do when this is the case? Well, there's no terribly easy answer. Typically,
    you'll need to download an HTTP request listener (for browsers like Chrome and
    Firefox, simply search for an HTTP request listener add-on). This will then allow
    you to see what requests are being made (both GET and POST requests), as well
    as the parameters that were passed. Once you know what the parameters are, then
    the rest works just like a GET request.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一些网站通过POST请求发送请求。不同之处在于，在POST请求中，参数嵌入在请求中（而不是嵌入在URL中）。这样，潜在的私人数据就不会在URL中明显显示（尽管这只是使用POST请求的一个用例）。那么当这种情况发生时，我们应该怎么做呢？嗯，没有特别简单的方法。通常，你需要下载一个HTTP请求监听器（对于像Chrome和Firefox这样的浏览器，只需搜索HTTP请求监听器插件）。这将允许你看到正在进行的请求（包括GET和POST请求），以及传递的参数。一旦你知道了参数是什么，其余的工作就像GET请求一样进行。
- en: 'Now, once we have our URL, the next step is to get the underlying source code
    and structure it. Of course, this can be a pain to do yourself, but fortunately,
    there are libraries out there which will clean and structure the source code for
    us. The one that I most frequently use is called **HtmlCleaner** and can be found
    at the following URL:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们有了URL之后，下一步就是获取底层的源代码并将其结构化。当然，自己来做这件事可能会很痛苦，但幸运的是，有一些库可以帮助我们清理和结构化源代码。我经常使用的一个库叫做**HtmlCleaner**，可以在以下URL找到：
- en: '[http://htmlcleaner.sourceforge.net/](http://htmlcleaner.sourceforge.net/)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://htmlcleaner.sourceforge.net/](http://htmlcleaner.sourceforge.net/)'
- en: It's a great library that gives you methods for cleaning and structuring the
    source code, navigating the resulting XML document, and ultimately parsing the
    values and attributes of the XML nodes. Once our data is cleaned, the last step
    is simply to walk through the tree and pick out the pieces of data we want. Now,
    this is easier said than done, as there's no really easy way to traverse the tree
    methodically and reliably using just Java and its native packages. What I mean
    by methodically and reliably is being able to traverse the tree and parse the
    correct data even when the structure of the underlying source code has changed
    slightly.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很棒的库，它提供了清理和结构化源代码的方法，导航生成的XML文档，最终解析XML节点的值和属性。一旦我们的数据被清理，最后一步就是简单地遍历树并挑选出我们想要的数据片段。现在，说起来容易做起来难，因为仅使用Java及其本地包并没有真正简单的方法来系统地和可靠地遍历树。我所说的系统性和可靠性是指即使底层源代码的结构稍微有所变化，也能够遍历树并解析正确的数据。
- en: For instance, say your parsing method was as naive as telling your code to give
    you the value of the fifth node. What happens then, when Yahoo! (or whatever site
    you're scraping) decides to add a new header to their website, and now the fifth
    node becomes the sixth? Even under this relatively simple change to the underlying,
    your scraper will break and will start returning you values from an incorrect
    node, and so ideally, we'd like to find a method for getting the correct node
    value regardless of how the underlying website changes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你的解析方法简单到告诉代码获取第五个节点的值。那么当Yahoo!（或你正在抓取的任何网站）决定在其网站上添加一个新标题，现在第五个节点变成了第六个节点时，会发生什么？即使在这种对底层网站的相对简单的更改下，你的爬虫也会崩溃，并开始从错误的节点返回值，因此，我们理想上希望找到一种方法，无论底层网站如何变化，都能获取正确的节点值。
- en: 'Luckily for us, oftentimes frontend engineers will build websites where important
    fields will have tags that contain either `class` or `id` attributes with unique
    values. We can then take advantage of these helpful and descriptive naming conventions
    and use a nifty language called **XPath**. The language itself is fairly self-explanatory
    once you see it; in fact, the syntax resembles that of any path (that is, directory
    path, URL path, and so on), so I''ll simply direct you to the following URL to
    learn the ins and outs, if you wish:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，前端工程师经常会构建这样的网站：重要字段会拥有带有唯一值的`class`或`id`属性的标签。我们可以利用这些有帮助的、描述性的命名约定，使用一种名为**XPath**的便捷语言。一旦你了解了它，这门语言本身是相当容易解释的；实际上，它的语法类似于任何路径（即目录路径、URL路径等），所以如果你愿意，我可以直接让你访问以下URL来了解其细节：
- en: '[http://www.w3schools.com/xpath/](http://www.w3schools.com/xpath/)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.w3schools.com/xpath/](http://www.w3schools.com/xpath/)'
- en: In any case, for now just keep in mind that XPath is a simple language that
    allows you to return sets of nodes which are determined by a path. What's special
    about XPath is that within the path, you can further refine your search by including
    various filters, ones that allow us to return only those `div` that are of a certain
    `class` for instance. This is where having descriptive `class` and `id` attributes
    comes in handy because we can drill into the HTML and efficiently find only those
    nodes that are important to us. Furthermore, if you still need additional weapons
    to parse the resulting XML, you could include regular expressions (REGEX) to help
    you in your search.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，现在只需记住XPath是一种允许你通过路径返回节点集的简单语言。XPath的特殊之处在于，在路径中，你可以通过包含各种过滤器来进一步细化搜索，例如，只返回特定`class`的`div`。这就是具有描述性的`class`和`id`属性发挥作用的地方，因为我们可以深入HTML中，高效地找到只对我们重要的节点。此外，如果你还需要额外的工具来解析结果XML，你可以使用正则表达式（REGEX）来帮助你搜索。
- en: In the end, the idea is to be as robust as possible with your parsing, as the
    last thing you want to do is to have to update your scrapers constantly as small,
    insignificant changes are made to the underlying website. Again, sometimes the
    website changes dramatically and you'll legitimately have to update your scraper,
    but the idea is to write them, again, as robustly as possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，目标是要使解析尽可能健壮，因为你们最不想做的就是随着底层网站的微小、不重要更改而不断更新爬虫。同样，有时网站会进行重大更改，你确实需要更新爬虫，但目标是要尽可能健壮地编写它们。
- en: 'At this point I''m sure you have plenty of questions. What does the code actually
    look like? How do you grab a website''s HTML? How do you even use the `HtmlCleaner`
    library? What''s an example of XPath? Previously, my goal was to lead you to a
    high-level understanding of what web scraping is, and along the way, I introduced
    a lot of different technologies and techniques that one would use. Now, let''s
    get our hands dirty with some code and see each of the preceding steps in action.
    Here are steps one and two for scraping our Blockbuster video games data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我相信你们一定有很多问题。代码实际上长什么样？如何获取一个网站的HTML？如何使用`HtmlCleaner`库？XPath的一个例子是什么？之前，我的目标是引导你们理解什么是网络爬虫，在此过程中，我介绍了很多不同的技术和方法。现在，让我们通过一些代码实操，看看前面的每个步骤。以下是抓取我们Blockbuster视频游戏数据的步骤一和步骤二：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So first we have a simple convenience class that allows us to get the source
    code of a passed-in URL. It simply opens a connection, sets a few standard web
    parameters, and then reads the input stream. We use a `StringBuilder` to efficiently
    construct one large string containing each line of the input stream, and finally
    close all connections and return the string. This string will then be the underlying
    HTML of the passed-in URL, and is what we''ll need in the next step to construct
    a clean, organized XML document. The code for that is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有一个简单的便捷类，可以获取传入URL的源代码。它只是打开一个连接，设置一些标准的网页参数，然后读取输入流。我们使用`StringBuilder`高效地构建一个包含输入流每一行的大字符串，并最终关闭所有连接并返回该字符串。这个字符串将是传入URL的底层HTML，也是下一步构建干净、有序的XML文档所需的。下面是相应的代码：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And so here we first write a simple method which allows us to connect to the
    resulting URL and grab its underlying source code. We then take that result and
    pass it to a cleaning method which instantiates a new instance of our `HtmlCleaner`
    class and calls the `clean()` method. This method will structure the underlying
    HTML into a well-formed XML document, and return the root of the XML as a `TagNode`
    object. The last step is simply looking at the underlying source code, determining
    what the correct XPaths are, and then running those over the given root `TagNode`.
    The abridged source code of Blockbuster''s video game rental page looks like the
    following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先编写一个简单的方法，允许我们连接到结果URL并获取其底层的源代码。然后我们取那个结果并传递给一个清理方法，该方法实例化我们`HtmlCleaner`类的新实例并调用`clean()`方法。这个方法将结构化底层的HTML成为一个格式良好的XML文档，并返回XML的根作为一个`TagNode`对象。最后一步只是查看底层的源代码，确定正确的XPath是什么，然后在给定的根`TagNode`上运行这些XPath。Blockbuster视频游戏租赁页面的缩略源代码如下所示：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: However, note that that this source code is as of the date I'm writing this
    and is not guaranteed to remain the same. However, from this source code above,
    we can see that each game is listed in a `div` tag with class `addToQueueEligible
    game sizeb gb6 bvr-gamelistitem`. This is somewhat of a long class name but we
    can have some confidence that by searching for `divs` with this class tag, we'll
    find video games and only video games because of how the class tag involves adding
    eligible games to a queue.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但是请注意，这段源代码是截至我撰写本文时的，不能保证保持不变。然而，从上面的源代码中，我们可以看到每个游戏都列在一个带有类`addToQueueEligible
    game sizeb gb6 bvr-gamelistitem`的`div`标签中。这是一个相当长的类名，但我们可以确信，通过搜索带有这个类标签的`divs`，我们会找到视频游戏，而且只有视频游戏，因为类标签涉及到将符合条件的游戏添加到队列中。
- en: 'Now, once we get to those desired `divs`, we see that the nodes we want are
    simply the first `a` node, as well as that `a` node''s corresponding `img` tag.
    Hence, in order to get the title and image URLs, respectively, our desired XPaths
    should look as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦我们找到了那些想要的`divs`，我们会发现我们需要的节点只是第一个`a`节点，以及该`a`节点的相应`img`标签。因此，为了分别获取标题和图片URL，我们想要的XPath应该如下所示：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With this, let''s now take a look at the full code of our scraper:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，现在让我们看一下我们抓取器的完整代码：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And that''s it! Most of this code we''ve already seen earlier, so really it''s
    just the `grabGamesWithTag()` method that we should hone in on. The first part
    of the method is to take the HTML patterns that we saw earlier (in the source
    code of the website) and combine them with our XPath formats. At this point, we
    have a valid XPath that will lead us to both the titles of the video games, as
    well as to the image URLs of the video games. The method from `HtmlCleaner` that
    we need to use to actually run this XPath command is as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们之前已经看过了大部分代码，所以实际上我们只需要关注`grabGamesWithTag()`方法。该方法的第一部分是将我们之前看到的HTML模式（网站的源代码）与我们的XPath格式结合起来。此时，我们有一个有效的XPath，它将引导我们找到视频游戏的标题以及视频游戏的图片URL。我们需要使用`HtmlCleaner`中的方法来实际运行这个XPath命令，如下所示：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will return a list of `Objects` which can then be cast to individual `TagNode`
    objects. What we need to do then is loop through each `Object` in our array, cast
    it to a `TagNode`, and extract either the value of the node or an attribute of
    the node to obtain the desired data. We can see that in the following part of
    the method:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个`Objects`列表，然后可以将其转换为单独的`TagNode`对象。然后我们需要做的是遍历数组中的每个`Object`，将其转换为`TagNode`，并提取节点的值或属性以获取所需的数据。我们可以在方法以下部分看到这一点：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In both cases here, the values that we need are specific attributes of the
    node, as opposed to the value of the node. Had it been a value, our code would
    have looked more like the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们需要的是节点的特定属性值，而不是节点的值。如果是一个值的话，我们的代码看起来会更像下面这样：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point, we've run through a quick primer on web scraping. Again, web
    scraping is a technique and an art that will take time to get used to and master,
    but is a great skill to have and is one that will open up countless opportunities
    for mining data across the Web. For now, focus on the concepts that were introduced
    in this chapter, as opposed to the actual code. The reason I say this is because
    how your code looks will very much depend on what web page you're trying to scrape.
    What won't change are the concepts behind the scraping, and so use those three
    steps mentioned in this chapter as a guide to how you can write a scraper for
    any web page.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经快速了解了网络爬取的基本知识。再次强调，网络爬取是一种技术和艺术，需要时间去适应和掌握，但它是一项非常棒的技术，可以为你打开无数的网络数据挖掘机会。现在，关注本章介绍的概念，而不是实际的代码。之所以这样说，是因为你的代码看起来会很大程度上取决于你试图爬取的网页。不会改变的是爬取背后的概念，因此使用本章提到的三个步骤作为指导，你可以为任何网页编写爬虫。
- en: Extending HTTP servlets for GET/POST methods
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展HTTP servlet以支持GET/POST方法
- en: Now that we have our web scraper written, we need a way to take the `VideoGame`
    objects that are returned, and actually store them in our database. Furthermore,
    we need a way to communicate with our server once it's up and running and tell
    it to scrape the site and insert it into our JDO database. Our gateway for communicating
    with our server is through what's called an HTTP servlet — something that we briefly
    mentioned earlier in the book.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经编写好了网络爬虫，我们需要一种方法来处理返回的`VideoGame`对象，并将它们实际存储到我们的数据库中。此外，我们还需要一种方式与服务器通信，一旦服务器启动并运行，告诉它去抓取网站内容并插入到我们的JDO数据库中。我们与服务器通信的网关是通过所谓的HTTP
    servlet，这在本书前面简要提到过。
- en: 'Setting up your backend in this way will be especially useful when we talk
    later about CRON jobs which, in order to automatically run some kind of function,
    require a servlet to communicate with (more on this soon). For now though, let''s
    see how we can extend the `HttpServlet` class and implement its `doGet()` method,
    which will listen and handle all HTTP GET requests sent to it. But first, what
    exactly is an HTTP GET request? Well, an HTTP web request is simply a user making
    a request to some server that will be sent over the network (that is, the Internet).
    Depending on the type of request, the server will then handle and send an HTTP
    response back to the user. There are then two common types of HTTP requests:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式设置后端将在我们稍后讨论CRON作业时特别有用，这些作业为了自动运行某种功能，需要一个servlet与之通信（关于这一点我们很快会详细介绍）。现在，让我们看看如何扩展`HttpServlet`类并实现其`doGet()`方法，该方法将监听并处理所有发送给它的HTTP
    GET请求。但首先，HTTP GET请求到底是什么？实际上，HTTP网络请求只是用户向某个服务器发起的请求，并通过网络（即互联网）发送。根据请求类型，服务器将处理并回送HTTP响应给用户。然后，有两种常见的HTTP请求类型：
- en: GET request — web requests that are only meant to retrieve data. These web requests
    will typically ask the server to query for some kind of data to be returned.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GET请求——仅用于检索数据的网络请求。这些请求通常会要求服务器查询并返回某种数据。
- en: POST request — web requests that submit data to be processed. Typically, this
    will ask the server to insert some kind of data that was submitted by the user.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: POST请求——提交数据处理的网络请求。通常，这会要求服务器插入用户提交的某种数据。
- en: 'In this case, since we aren''t getting any data for a user or submitting any
    data from a user (in fact we''re not really interacting with any users at all),
    it really doesn''t make a difference which type of request we use, so we''ll stick
    with the simpler GET request as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于我们既不需要获取用户数据，也不需要提交任何用户数据（实际上我们根本不与任何用户交互），使用哪种类型的请求实际上并没有区别，因此我们将使用更简单的GET请求，如下所示：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So the method itself is quite simple. We already have our `getVideoGamesByConsole()`
    method from earlier, which goes and does all the scraping, returning a list of
    `VideoGame` objects as a result. We then simply run it for every console that
    we want, and at the end use our nifty JDO database wrapper class and call its
    `batchInsertGames()` method for quicker insertions. Once that''s done, we take
    the HTTP response object that is passed in and quickly write some kind of message
    back to the user to let them know whether or not the scraping was successful.
    In this case, we don''t make use of the `HttpServletRequest` object that gets
    passed in, but that object will come in very handy if the requester passes parameters
    into the URL. For instance, say you wanted to write your servlet in a way that
    only scrapes one specific game platform instead of all of them. In that case,
    you would need some way of passing a p latform-type parameter to your servlet,
    and then extracting that passed-in parameter value within the servlet. Well, just
    like how earlier we saw that Yahoo! Finance allows you to pass in tickers with
    key value `s`, to pass in a platform type, we could simply do the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个方法本身非常简单。我们之前已经有了`getVideoGamesByConsole()`方法，它会执行所有抓取操作，并返回一个`VideoGame`对象列表作为结果。然后，我们只需针对想要的所有游戏机运行它，最后使用我们巧妙的
    JDO 数据库包装类，并调用其`batchInsertGames()`方法以快速插入数据。完成这些后，我们获取传入的 HTTP 响应对象，并快速向用户编写一些信息，以告知他们抓取是否成功。在这种情况下，我们没有使用传入的`HttpServletRequest`对象，但如果请求者在
    URL 中传递参数，这个对象将非常有用。例如，假设你想以只抓取一个特定的游戏平台而非所有平台的方式来编写 Servlet。在这种情况下，你需要某种方式将平台类型参数传递给
    Servlet，并在 Servlet 中提取传入的参数值。正如我们之前看到的雅虎财经允许你使用键值对`s`传递股票代码一样，为了传递平台类型，我们可以简单地执行以下操作：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, on the servlet side do:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 Servlet 端执行以下操作：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Pretty simple, right? You just have to make sure that the key used in the URL
    matches the parameter you request within the servlet class. Now, the last and
    final step for getting this all hooked together is defining the URL path in your
    GAE project — namely, making sure your GAE project knows that the URL pattern
    actually points to this class you just wrote. This can be found in your GAE project''s
    `/war/WEB-INF/` directory, specifically in the `web.xml` file. There you''ll need
    to add the following. To make sure that the servlet name and class path matches
    the given URL pattern:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单，对吧？你只需要确保 URL 中使用的键与 Servlet 类中请求的参数相匹配。现在，为了将所有这些连接在一起，最后一步是在你的 GAE 项目中定义
    URL 路径——即确保你的 GAE 项目知道 URL 模式实际上指向你刚刚编写的这个类。这可以在你的 GAE 项目的`/war/WEB-INF/`目录中找到，具体是在`web.xml`文件中。你需要在其中添加以下内容，以确保
    Servlet 名称和类路径与给定的 URL 模式相匹配：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At this point, we have our scraper, we have our JDO database, and we even have
    our first servlet all hooked up and ready to go. The last part is scheduling your
    scraper to run periodically; that way, your database has the latest and most up-to-date
    data, without you having to sit in front of your computer every day and manually
    call your scraper. In this next section, we'll see how we can use CRON jobs to
    accomplish just this.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有了抓取器，我们有了 JDO 数据库，甚至我们的第一个 Servlet 也已经连接并准备就绪。最后一部分是定期安排你的抓取器运行；这样，你的数据库就有最新的、最及时的数据，而不需要你每天坐在电脑前手动调用你的抓取器。在下一节中，我们将看到如何使用
    CRON 作业来实现这一点。
- en: Scheduling CRON jobs
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安排 CRON 作业
- en: 'First, let''s define what a CRON job is. The term **cron** originally referred
    to a time-based job scheduler in Unix that allowed you to schedule jobs/scripts
    to be run periodically at specific times. The same concept can be applied to web
    requests, and in our case, the goal is to run our web scraper and update the data
    in our database periodically and without our interference. Another reason why
    GAE is so convenient to use is because of how easy the platform makes scheduling
    CRON jobs. To do so, we simply need to create a `cron.xml` file in the `/war/WEB-INF/`
    directory of our GAE project. In this XML file, we add the following code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一下 CRON 作业是什么。**cron**这个术语最初指的是 Unix 中基于时间的工作调度程序，允许你安排作业/脚本在特定时间定期运行。同样的概念可以应用于网页请求，而在我们的情况下，目标是定期运行我们的网页抓取器并更新数据库中的数据，而无需我们的干预。GAE
    平台之所以方便使用，另一个原因是它让安排 CRON 作业变得非常简单。为此，我们只需在 GAE 项目的`/war/WEB-INF/`目录中创建一个`cron.xml`文件。在这个
    XML 文件中，我们添加以下代码：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is pretty self explanatory. First, we define root tags named`<cronentries>`
    and within these, we can insert any number of`<cron>` tags — each one denoting
    a scheduled process. In these`<cron>` tags, we need to tell the scheduler what
    the URL that we want to hit is (this will be relative to the root URL, of course),
    as well as the schedule itself (in our case, it's everyday at 12:50 A.M.). Other
    optional tags are a description tag, a time-zone tag, and/or a target tag that
    allows you to specify which version of your GAE project to invoke the specified
    URL.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于是自解释的。首先，我们定义了一个名为`<cronentries>`的根标签，在这些标签内，我们可以插入任意数量的`<cron>`标签——每个标签都表示一个计划中的进程。在这些`<cron>`标签中，我们需要告诉调度程序我们想要访问的URL是什么（这当然会相对于根URL），以及计划本身（在我们的案例中，是每天凌晨12:50）。其他可选标签有描述标签、时区标签和/或目标标签，允许你指定调用指定URL的GAE项目的哪个版本。
- en: 'Now, in my case, I asked the scheduler to run the job every day at 12:50 A.M.
    (PST), but examples of other schedule formats are as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的案例中，我让调度程序每天太平洋标准时间凌晨12:50运行该任务，但其他调度格式的例子如下：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'I won''t go into the exact syntax of the scheduler tags, but you can see that
    it''s pretty intuitive. However, for those of you who would like to learn more
    about CRON jobs in GAE or look at some of the less commonly used features, feel
    free to check out the following URL for a comprehensive look at CRON jobs:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会深入讲解调度标签的确切语法，但你可以看出它非常直观。然而，对于那些想要在GAE中了解有关CRON作业更多信息或者查看一些较少使用的功能的人，可以随时查看以下URL以全面了解CRON作业：
- en: '[http://code.google.com/appengine/docs/java/config/cron.html](http://code.google.com/appengine/docs/java/config/cron.html)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://code.google.com/appengine/docs/java/config/cron.html](http://code.google.com/appengine/docs/java/config/cron.html)'
- en: But as far as our example goes, what we did previously will suffice and so we'll
    stop here!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但就我们的示例而言，我们之前所做的工作已经足够，因此我们就此打住！
- en: Summary
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we yet again covered a lot of ground. We started off the chapter
    simply looking at various ways to collect data. In some cases, convenient APIs
    released by other companies are readily available for us to use and query (though
    one must be careful about legal issues when it comes to storing that data). However,
    many times we'll find ourselves needing to go out and grab that data ourselves,
    and this can be done through web scraping.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们再次涉猎了很多内容。我们从探讨收集数据的各种方法开始本章。在有些情况下，其他公司发布的便捷API可供我们使用和查询（尽管在存储这些数据时必须注意法律问题）。然而，很多时候我们需要自己出去抓取数据，这可以通过网页抓取完成。
- en: In the next section, we went through a primer on web scraping — starting with
    the high-level concepts behind what web scraping is and what steps you need to
    take to perform it, and ending with the implementation. The example we went through
    involved scraping Blockbuster's site for the latest video games available for
    rent, and in the process, we wrote our first XPath expressions and implemented
    our first HTTP servlet.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们通过一个网页抓取入门教程进行了学习——从网页抓取是什么以及执行抓取需要采取哪些步骤的高层次概念开始，到具体实现结束。我们通过抓取Blockbuster网站获取可供租借的最新视频游戏为例进行了学习，在这个过程中，我们编写了我们的第一个XPath表达式并实现了第一个HTTP
    servlet。
- en: While implementing our HTTP servlet, we briefly discussed the two common types
    of HTTP requests (GET and POST requests) and proceeded to implement an HTTP GET
    request that would allow us to call our video game scraper class, collect the
    aggregated `VideoGame` objects, and then insert them into our JDO database using
    our convenient wrapper class from the previous chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现我们的HTTP servlet时，我们简要讨论了两种常见的HTTP请求类型（GET和POST请求），然后实现了一个HTTP GET请求，这将允许我们调用我们的视频游戏抓取器类，收集聚合的`VideoGame`对象，并使用前一章中的便捷包装类将它们插入到我们的JDO数据库中。
- en: Finally, we ended the chapter by looking at ways in which we could schedule
    the scraping of Blockbuster's site in order to ensure the latest and most up-to-date
    data, without having to manually call the scraper ourselves every day. We introduced
    a special technology known as CRON jobs and implemented one using the GAE platform.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过探讨如何安排对Blockbuster网站的抓取来结束本章，以确保获取最新和最及时的数据，而无需我们每天手动调用抓取器。我们介绍了一种称为CRON作业的特殊技术，并使用GAE平台实现了一个。
- en: In the next and last chapter, we'll try to bring everything we learned together.
    More specifically, now that the data collection and insertion parts of our system
    are up and running, we'll implement a few more servlets that will allow us to
    make an HTTP GET request and retrieve various types of data. Then, we'll go through
    the client side of the code, and look at how you can make these GET requests from
    the Android application and parse the response for the desired data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章也是最后一章中，我们将尝试把所学的一切融合在一起。更具体地说，既然我们系统的数据收集和插入部分已经运行起来了，我们将实现几个额外的servlet，使我们能够发起HTTP
    GET请求并检索各种类型的数据。接下来，我们将研究代码的客户端部分，看看如何从Android应用程序发起这些GET请求并解析响应以获取需要的数据。
