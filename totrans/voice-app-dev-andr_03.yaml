- en: Chapter 3. Speech Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Have you ever tapped through several menus and options on your device until
    you were able to do what you wanted? Have you ever wished you could just say some
    words and make it work? This chapter looks at **Automatic Speech Recognition**
    (**ASR**), the process that converts spoken words to written text. The topics
    covered are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The technology of speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Google speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing applications with the Google speech recognition API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have a good understanding of the issues
    involved in using speech recognition in an app and should be able to develop simple
    apps using the Google speech API.
  prefs: []
  type: TYPE_NORMAL
- en: The technology of speech recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the two main stages in speech recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Signal processing**: This stage involves capturing the words spoken into
    a microphone and using an **analogue-to-digital converter** (**ADC**) to translate
    it into digital data that can be processed by the computer. The ADC processes
    the digital data to remove noise and perform other processes such as echo cancellation
    in order to be able to extract those features that are relevant for speech recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition**: The signal is split into minute segments that are matched
    against the phonemes of the language to be recognized. Phonemes are the smallest
    unit of speech, roughly equivalent to the letters of the alphabet. For example,
    the phonemes in the word cat are */k/*, */æ/*, and */t/*. In English, for example,
    there are around 40 phonemes, depending on which variety of English is being spoken.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most successful approach to speech recognition has been to model speech
    statistically so that the outcome of the process is a series of guesses (or hypotheses)
    as to what the user might have said, ranked according to the computed probability.
    Complex probabilistic functions are used to perform this statistical modeling.
    The most commonly used among these is the *Hidden Markov Model*, but neural networks
    are also used and in some cases, hybrid processes using a combination of these
    two approaches. The models are tuned through a process of training using large
    amounts of training data. Usually, hundreds to thousands of audio hours are employed
    in order to handle the variability and complexity of human speech. The result
    is an *acoustic model* that represents the different ways in which the sounds
    and words of a language can be pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: On its own, the acoustic model is not sufficient for high performance speech
    recognition. An additional part of a speech recognition system is another statistical
    model, the *language model*. This model contains knowledge about permissible sequences
    of words and which words are more likely in a given sequence. For example, although
    acoustically the words *to* and *two* sound the same, the former is more likely
    in the phrase *I went to the shops* and the latter in the phrase *I bought two
    shirts*. The language model would help to return the correct word in each phrase.
  prefs: []
  type: TYPE_NORMAL
- en: The output of speech recognition is a list of recognition results (known as
    the *N-best list*) ranked according to the confidence the recognizer has that
    the recognition is correct, in an interval from 0 to 1\. A value close to 1.0
    indicates high confidence that the recognition is correct, while values closer
    to 0.0 indicate low confidence. The N-best list and the confidence scores are
    useful as it may be the case that the first-best recognized string is not what
    the user actually said and that the alternatives might offer a result that is
    more appropriate. The confidence scores are also useful as they can provide a
    basis for deciding how the system should continue, for example, whether to confirm
    the recognized phrase or not.
  prefs: []
  type: TYPE_NORMAL
- en: Using Google speech recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Speech recognition services have been available on Android devices since Android
    2.1 (API level 7). One place where recognition is available is via a microphone
    icon on the Android keyboard. Clicking on the microphone button activates the
    Google speech recognition service, as shown in the following screenshot. The red
    microphone and the textual prompt indicate that the system is expecting some speech:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Google speech recognition](img/5297_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If no speech is detected, a re-prompt dialog is generated, asking the user to
    try to speak again. Another possible situation is where no suitable match can
    be found for the user's spoken input. In this case, the screen displays the message
    **No matches found**. Finally, the message **Connection problem** is displayed
    when no internet connection is available.
  prefs: []
  type: TYPE_NORMAL
- en: The parameters of the on-board speech recognition services can be adjusted either
    through **Settings** | **Language and input** | **Speech** | **Voice Search**
    or, depending on the device, by clicking on the **Tool** icon at the top hand
    corner of the screen. A number of available languages will be presented. Checking
    for the availability of languages and also selecting a language can be done programmatically.
    This will be covered in [Chapter 7](ch07.html "Chapter 7. Multilingual and Multimodal
    Dialogs"), *Multilingual and Multimodal Dialogs*.
  prefs: []
  type: TYPE_NORMAL
- en: Developing applications with the Google speech recognition API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The components of the Google speech API (package `android.speech`) are documented
    at [http://developer.android.com/reference/android/speech/package-summary.html](http://developer.android.com/reference/android/speech/package-summary.html).
    Interfaces and classes are listed here and further details can be obtained by
    clicking on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways in which speech recognition can be carried out on an Android
    device: based solely on a `RecognizerIntent` approach, or by creating an instance
    of `SpeechRecognizer`. The former provides an easy-to-program mechanism with which
    it is possible to create apps that use speech recognition by starting the Intent
    class and processing its results. The apps following this scheme will present
    a dialog providing feedback to the user on whether the ASR is ready, or about
    different errors during the recognition process. Using `SpeechRecognizer` provides
    developers with different notifications of recognition-related events, thus allowing
    a more fine-grained processing of the speech recognition process. With this approach
    the dialog is not shown, providing more control for the developer on the app''s
    GUI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we will present two apps (`ASRWithIntent` and `ASRWithLib`)
    that recognize what the user says and present the results for the recognition
    in the form of an N-best list with confidence scores. In essence, they are the
    same application but they have been developed following the two different approaches
    described: `ASRWithIntent` uses the `RecognizerIntent` approach, and `ASRWithLib`
    the `SpeechRecognizer` approach which we have programmed in the ASRLib library
    (`sandra.libs.asr.asrlib` in the code bundle).'
  prefs: []
  type: TYPE_NORMAL
- en: ASRWithIntent app
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This simple app illustrates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The user selects the parameters for speech recognition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user presses a button and says some words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The words recognized are displayed in a list along with their confidence scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the opening screen, there is a button with the message **Press the button
    to speak**. When the user presses the button, speech recognition is launched using
    the parameters selected by the user. The results of an instance of activating
    speech recognition and saying the words *What''s the weather like in Belfast*
    are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ASRWithIntent app](img/5297_03_02_correct.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The button that the user presses to start speech recognition is set up through
    reference to the button specified in `asrwithintent.xml`, which you can find in
    the code bundle (in the `res/layout` folder of the `ASRWithIntent` project):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When the user presses the button, the method `listen()` is invoked in which
    the details of `RecognizerIntent` are set. In `ASRWithIntent`, speech recognition
    is supported by the class `RecognizerIntent` by sending an Intent with the action
    `ACTION_RECOGNIZE_SPEECH`, using the method `startActivityForResult(Intent,int)`,
    where the `int` value is a request code defined by the developer. If it is greater
    than 0, this code will be returned in `onActivityResult()` when the activity exits.
    The request code serves as an identifier to distinguish which intent generated
    a certain result from the different intents that could have been invoked within
    the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code starts an activity to recognize speech:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the code, there are a number of extras associated with `ACTION_RECOGNIZE_SPEECH`.
    One of these, `EXTRA_LANGUAGE_MODEL`, is required. The others are optional. As
    its name implies, `EXTRA_LANGUAGE_MODEL` specifies the language model to be used
    in the recognition process. The following two options are supported by it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LANGUAGE_MODEL_FREE_FORM`: This language model is based on free-form speech
    recognition and is used to recognize free-form speech, for example, in the dictation
    of an e-mail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LANGUAGE_MODEL_WEB_SEARCH`: This language model is based on web search terms
    and is used to model more restricted forms of input such as shorter, search-like
    phrases, for example, *flights to London*, *weather in Madrid*, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As can be observed, both options imply quite unrestricted inputs. To build an
    app in which only certain keywords are accepted, or to use recognition grammars,
    currently it is necessary to post-process the recognition results to match them
    against the expected patterns. Some examples on how to do this will be shown in
    [Chapter 6](ch06.html "Chapter 6. Grammars for Dialog"), *Grammars for Dialog*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other extras are described in the Android documentation for the `RecognizerIntent`
    class. The following are some of the more commonly used optional extras:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EXTRA_PROMPT`: This provides a text prompt that is shown to users when they
    are asked to speak.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EXTRA_MAX_RESULTS`: This integer value specifies a limit on the maximum number
    of results to be returned. If omitted, the recognizer will choose how many results
    to return. The results are the different possible texts corresponding to the user''s
    input and sorted from most to less probable (the N-best list discussed earlier
    in this chapter).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EXTRA_LANGUAGE`: This specifies a language that can be used instead of the
    default provided on the device. The use of other languages is covered in [Chapter
    7](ch07.html "Chapter 7. Multilingual and Multimodal Dialogs"), *Multilingual
    and Multimodal Dialogs*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user can be asked to choose from these options or they can be set programmatically.
    The `ASRWithIntent` app prompts the user for the language model and the maximum
    number of results. If the information is not provided, it uses the default values
    indicated in two constants (see the `showDefaultValues` and `setRecognitionParams`
    methods in the `asrwithintent.java` file).
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the speech recognition are returned via activity results in
    `onActivityResults(int, int, Intent)`. At this point, the recognition is complete.
    However, normally we would want to see the results or use them in some way. The
    additional steps required for this are illustrated in the following annotated
    code from `ASRWithIntent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As discussed earlier, the recognition results are returned through the intent.
    The matched sentences are accessible using the method `getStringArrayListExtra`
    using `RecognizerIntent.EXTRA_RESULTS` as a parameter. Similarly, an array with
    the confidence scores can be obtained with `getFloatArrayExtra` and `RecognizerIntent.EXTRA_CONFIDENCE_SCORES`.
  prefs: []
  type: TYPE_NORMAL
- en: The results are held in a new `ArrayList<String>`, the elements of which combine
    the strings from the matches and scores (represented as strings) from the float
    array. Note that there is a possibility that the confidence score is -1; this
    is the case when the confidence score is unavailable. Then, `ArrayAdapter` is
    called in the `setListView` method to insert the formatted strings into the `ListView`
    method of the GUI.
  prefs: []
  type: TYPE_NORMAL
- en: 'It might also happen that the recognition could not be carried out satisfactorily
    (`resultCode!=RESULT_OK` in the previous code). There are different constants
    for the main error situations defined in the `RecognizerIntent` class: `RESULT_AUDIO_ERROR`,
    `RESULT_CLIENT_ERROR`, `RESULT_NETWORK_ERROR`, `RESULT_SERVER_ERROR`, and `RESULT_NOMATCH`.
    They all correspond to errors due to the physical device or the network, except
    for the last one that corresponds to the situation in which no phrase matched
    the audio input. Developers can use the result code to carry out a detailed treatment
    of these errors. However, the dialog shown already implements a naive treatment
    of such errors which may suffice for simpler apps. In this case, the user receives
    feedback on the errors and when it is applicable, they are asked to repeat their
    utterance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, since the app needs to access the Internet to carry out speech recognition,
    it is necessary to set the permission in the `manifest` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ASRWithLib app
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ASRWithLib` app has exactly the same functionality as `ASRWithIntent`,
    but instead of solely using a `RecognizerIntent` class, it creates an instance
    of the `SpeechRecognizer` class. In order to make the code usable in all the apps
    that employ ASR, we propose to use a library as we did in the previous chapter
    for TTS. In this case, the library is in `sandra.libs.asr.asrlib` and contains
    only one class `ASR`, which implements the `RecognitionListener` interface, and
    thus all the methods that cope with the different recognition events namely `onResults`,
    `onError`, `onBeginningOfSpeech`, `onBufferReceived`, `onEndOfSpeech`, `onEvent`,
    `onPartialResults`, `onReadyForSpeech`, and `onRmsChanged`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SpeechRecognizer` instance is created in the `createRecognizer` method
    after checking that the ASR engine is available in the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code queries the `PackageManager` class to check whether recognition is
    supported. If the value of `intActivities`.size() is greater than zero, speech
    recognition is supported, and an instance of `SpeechRecognizer` is then created
    and saved in the attribute `myASR`. The listener for recognition is set using
    a reference to `this`, as the ASR class implements the `RecognitionListener` interface.
  prefs: []
  type: TYPE_NORMAL
- en: The `listen` method is used to start listening using a `RecognizerIntent` class.
    Although the extras are the same as in the `ASRWithIntent` app, the way in which
    the recognition is started is slightly different. `ASRWithIntent` used `startActivityForResult`
    while in this case, the `SpeechRecognizer` object is in charge of starting the
    recognition receiving the intent as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the `ASRWithIntent` app, all the code was in the same class and the testing
    for the correctness of the recognition parameters (`languageModel` and `maxResults`)
    was carried out in that single class. However, here we are building a library
    that will be used in many apps, thus it is not possible to take for granted that
    the parameters will be checked before invoking the `listen` method. This is why
    the method checks their values and throws an exception if they are not correct.
  prefs: []
  type: TYPE_NORMAL
- en: Several methods are used to react to the different events that can be raised
    by the ASR engine. It is not a good policy to implement the response to such events
    in the library, firstly because it would imply that all apps that use the library
    make the same management of these events; secondly because most of the time responding
    to the events involves showing messages to the user or using the GUI in some other
    way, and it is a basic design principle to separate the logic of the application
    from its interface.
  prefs: []
  type: TYPE_NORMAL
- en: That is why the `ASR` class employs abstract methods. Abstract methods only
    declare the header, and it is the responsibility of the subclasses to provide
    the code for them. This way, each of the methods in charge of responding to ASR
    events invokes an abstract method, and the behavior of such methods varies for
    each of the subclasses of ASR. In the `ASRWithLib` example, the `ASRWithLib` class
    is a subclass of ASR and implements the abstract methods in a certain way. If
    we had another app in which we wanted to develop a different behavior, we could
    write separate code for those methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the `onResults` method from the `ASR.java` class is invoked when
    the engine finds sentences that match what the user said. The code for this method
    in `ASR.java` is as follows. Note that to retrieve the results, it uses static
    methods from the `SpeechRecognizer` class, and not from `RecognizerIntent` as
    in `ASRWithIntent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This method invokes the abstract method `processAsrResults`. It is in the `ASRWithLib`
    class where this method is implemented, indicating how to process the results
    (in this case by populating a list view as in `ASRWithIntent`).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might have observed, the `ASRWithLib` app does not show the recognition
    dialog. This may be desirable in apps that perform continuous speech recognition
    (ASR is always active as a background service), for which such feedback can be
    annoying for the user. However, for other apps it is necessary to show some feedback
    to the user so that they know that the app is listening. This is carried out with
    the `onAsrReadyForSpeech` method. This method is executed when the ASR engine
    is ready to start listening, and is an abstract method from `ASRLib` implemented
    in `ASRWithLib.java` by changing the color and message of the speech button (both
    the text and color are not hard-coded but are retrieved from the `res/values`
    folder):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it is necessary to set the permissions in the `manifest` file to access
    the Internet for using ASR and to record audio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has shown how to use the Google speech API to implement speech
    recognition services, having checked that they are available on the device. The
    user is prompted to say some words and the results of the recognition, the recognized
    strings and their confidence scores, are displayed on the screen. The user can
    choose the language model for recognition and the maximum number of results to
    be retrieved. This functionality has been implemented following two different
    approaches in the `ASRWithIntent` and `ASRWithLib` apps.
  prefs: []
  type: TYPE_NORMAL
- en: The `ASRWithIntent` app is a basic easy-to-develop example in which all the
    code is contained in the same class. ASR is carried out using a `RecognizerIntent`
    class and there is an automatically generated dialog that provides feedback on
    whether the engine is listening or if there was any error.
  prefs: []
  type: TYPE_NORMAL
- en: The `ASRWithLib` app shows how to modularize and create a library for speech
    recognition that can be used in many apps. Instead of relying on the `RecognizerIntent`
    class only, it uses an instance of the `SpeechRecognizer` class and implements
    the `RecognitionListener` interface using abstract methods, providing a flexible
    implementation that can respond to a wide range of ASR events.
  prefs: []
  type: TYPE_NORMAL
- en: While these two examples are not particularly useful apps as such, the code
    presented here can be used virtually unchanged in any apps using speech recognition.
    Future examples in this book will be built on this code. The next chapter will
    show how TTS and ASR can be combined to perform simple voice interactions in which
    the user can ask for information or issue a command to the device.
  prefs: []
  type: TYPE_NORMAL
