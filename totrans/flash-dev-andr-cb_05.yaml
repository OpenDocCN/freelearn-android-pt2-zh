- en: 'Chapter 5. Rich Media Presentation: Working with Images, Video, and Audio'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading photographs from the device cameraRoll
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying Pixel Bender Shader effects to loaded images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing video files from the local file system or over HTTP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing remote video files over RTMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing audio files from the local file system or over HTTP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating an audio spectrum visualizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating audio tones for your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will include a variety of recipes for the display of image data
    and playback of both video and audio streams. Included among these recipes are
    examples demonstrating the ability to load images from the device camera repository,
    applying Pixel Bender Shaders to loaded images, the playback of audio and video
    over different protocols, as well as the generation of visual data from sound
    and the generation of raw sound data.
  prefs: []
  type: TYPE_NORMAL
- en: The Flash platform is well known as the premiere video distribution platform
    worldwide. In the following pages, we will see that this experience and reach
    is in no way confined to desktop and browser-based computing. With new features
    such as StageVideo available in AIR 2.6 and Flash Player 10.2, Flash is becoming
    an even stronger platform for delivering video while preserving device battery
    life and providing a better user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Loading photographs from the device cameraRoll
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Android operating system has a central repository for storing photographs
    captured by the variety of camera applications a user may have installed. There
    are APIs within AIR for Android, which allows a Flash developer to specifically
    target and pull from this repository for display within an application.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We must use the mobile `CameraRoll` API to browse directly to the device camera
    roll and select a photograph for display:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following classes into your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Declare a `CameraRoll` object and a `Loader`, which will be used to display
    the photograph, once selected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will create our `Loader` object, add it to the `Stage`, and register an
    event listener to properly scale the photo once it has been loaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the `CameraRoll` itself, all we need to do is instantiate it and then add
    an event listener to fire once the user has selected a photograph to display.
    We should always check to see whether the device supports `CameraRoll.browseForImage()by`
    checking the `supportsBrowseForImage` property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will now register a `TouchEvent` listener of type `TOUCH_TAP` to the `Stage`.
    This will enable the user to invoke a browse dialog in order to select a photograph
    from the `CameraRoll` by tapping the device screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: We are setting `Multitouch.inputMode` to the `MultitouchInputMode.TOUCH_POINT`
    constant in order for our application to accept touch events.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the following method is invoked from a user interaction, we can invoke
    the `browseForImage()` method upon the `CameraRoll` object we had set up earlier.
    This will open the default gallery application on an Android device and allow
    the user to select a photograph from their collection. If there is more than one
    gallery application on the device, the user will first choose which one to use
    for this event through a native Android dialog. Our application will lose focus
    and this will be handled by the operating system, returning to our application
    once a selection is made.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see the default gallery application on Android. A user can spend
    as much time as they wish browsing the various collections and photographs before
    a selection is made.![How to do it…](img/1420_05_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the user has performed a valid selection in the native Android gallery
    application, focus returns to our application and an event containing a `MediaPromise`
    object is returned. The `Loader` class has a specific method called `loadFilePromise()`
    specifically for this sort of thing. We will now pass the `MediaPromise` through
    this method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we've passed the `MediaPromise` object through the `Loader` using `loadFilePromise()`,
    it will load up onto the `Stage`. We will perform one more action here to adjust
    the `Loader` size to fit within the constraints of our `Stage:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting image, when loaded upon the `Stage`, will appear as follows:![How
    to do it…](img/1420_05_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ActionScript `CameraRoll` API specifically targets the on device storage
    location for photographs on Android. Whenever a user performs some interaction
    that invokes a `CameraRoll.browseForImage()` method in our application, the default
    Android gallery application will launch, allowing the user to select an image
    file from within their collection.
  prefs: []
  type: TYPE_NORMAL
- en: Once the user has selected a photograph from the gallery application, they will
    be returned to our AIR for Android application along with a `MediaPromise` object
    with which we can ascertain certain information about the file, or even load the
    photograph directly into our application.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we examine how to load an image from the `CameraRoll` into
    a `Loader` on the `Stage`. There are, of course, many things we could do to the
    photograph once it has been loaded up. For an example of this, have a look at
    the next recipe: *Applying Pixel Bender Shader effects to loaded images*.'
  prefs: []
  type: TYPE_NORMAL
- en: Applying Pixel Bender Shader effects to loaded images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we load a visual object into our application, as this is all Flash-based,
    we can do all sorts of robust visual manipulation. In this example, we will load
    a preselected photograph from the local file system, and then apply a variety
    of Pixel Bender Shaders to it, drastically changing its appearance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe makes use of Pixel Bender Shaders. You can download `.pbj` files
    from the Adobe Exchange or create your own.
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to write your own Pixel Bender kernels, you can download the Pixel
    Bender Toolkit for free from [http://www.adobe.com/devnet/pixelbender.html](http://www.adobe.com/devnet/pixelbender.html)
    and use it to compile all sorts of shaders for use in Flash and AIR projects.
  prefs: []
  type: TYPE_NORMAL
- en: The toolkit allows you to write kernels using the Pixel Bender kernel language
    (formerly known as Hydra) and provides mechanisms for image preview and separate
    property manipulation that can be exposed to ActionScript.
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready…](img/1420_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a good resource on writing Pixel Bender Shaders, check out the documentation
    located at [http://www.adobe.com/devnet/pixelbender.html](http://www.adobe.com/devnet/pixelbender.html).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are also referencing a photograph that exists within the
    Android image gallery, which we previously captured with the default camera application.
    You may do the same, or simply bundle an image file along with the application
    for later reference.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now load a predetermined image from the local device storage and apply
    multiple Pixel Bender Shaders to it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following classes into your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For this recipe, we must declare a number of different objects up front. We
    will declare a `String` constant to hold the path to our image and a `Loader`,
    which will be used to display the photograph. A `URLRequest` and `URLLoader` object
    pair will be used to load in our `.pbj` files. The `Array` will be set up to hold
    the names of each `.pbj` we will be loading. An `int` is employed to keep track
    of the shader we have currently loaded from our `Array` set. Finally, a `Shader`
    and `ShaderFilter` pair are declared to apply the loaded `.pbj` onto our `Loader`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to initialize our `Array` and populate it with the Pixel Bender
    Shader file references we will be loading into our application. These files can
    be obtained through the Adobe Exchange, other locations on the web, or authored
    using the Pixel Bender Toolkit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create our `Loader` object, add it to the `Stage`, and register an
    event listener to properly scale the photo once it has been loaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will now register a `TouchEvent` listener of type `TOUCH_TAP` to the `Loader`.
    This will enable the user to tap the loaded image to cycle through a variety of
    Pixel Bender Shaders. We also set the `currentFilter int` to `0`, which will indicate
    the first position of our `Array:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To load the photograph into the `Loader` instance for display within our application,
    we will invoke the `load()` method and pass in a new `URLRequest` along with the
    `photoURL String` constant that was declared earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the file has loaded, we will perform one more action to adjust the `Loader`
    size to fit within the constraints of our `Stage:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting image, when loaded upon the `Stage`, without any shaders applied,
    will appear as follows:![How to do it…](img/1420_05_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each time the users performs a touch tap upon the `Loader` instance, this method
    will execute. Basically, we are setting up a `URLRequest` using values from the
    `Array` of shader locations that was set up earlier, pulling the value from whatever
    current index that has been recorded to the `currentFilter` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we invoke the `URLLoader.load()` method, we must explicitly set the `dataFormat`
    property to the `URLLoaderDataFormat.BINARY` constant. This ensures that when
    our file is loaded up, it is treated as binary and not text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An `Event.COMPLETE` listener is registered to invoke the `applyFilter` method
    once our shader has been loaded up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we can either increment our `currentFilter` value, or set it back to
    `0`, depending upon where we are along the length of the `Array:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To actually apply the loaded `.pbj` onto our `Loader`, we will first assign
    the binary data to a new `Shader` object. This is subsequently passed through
    the constructor of a `ShaderFilter`, which is then applied to the filters property
    of our `Loader` as an `Array:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When the user has tapped the image, we cycle through the available Pixel Bender
    Shaders and apply then, in turn, to the loaded photograph. The resulting image
    cycle can be seen as follows:![How to do it…](img/1420_05_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Pixel Bender Shaders is a simple and direct way of enabling some really
    powerful visual manipulation within an application. In this recipe, we load an
    image into a `Loader` object, construct an `Array` of `.pbj` file references to
    pass through a `URLLoader`. When the user interacts with our loaded image, we
    will load a `.pbj` file and construct a `Shader` based upon the received data.
    Finally we can construct a `ShaderFilter` based off of this object and pass this
    onto our image through the `Loader.filters` property.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we examine how to load an image into a `Loader` on the `Stage`
    and look at applying Pixel Bender Shaders to it upon user interaction. You can,
    of course, apply such shaders to any `DisplayObject` you like, including video!
  prefs: []
  type: TYPE_NORMAL
- en: A good place to locate a variety of Pixel Bender files to use in such an example,
    is the Adobe Exchange. Visit the Exchange website at [http://www.adobe.com/exchange](http://www.adobe.com/exchange).
  prefs: []
  type: TYPE_NORMAL
- en: Playing video files from the local filesystem or over HTTP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have the full Flash Player (and Adobe AIR) on Android devices, playback
    of video files is as simple as it normally is on the desktop. The main consideration
    is whether the video is optimized for playback on mobile, or not.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe involves the playback of a video file that has been packaged along
    with our application. We could just as easily reference an HTTP address or even
    local storage on the Android device, so long as it is a file format and codec,
    which can be played back through Flash Platform runtimes. You will want to prepare
    this file ahead of time.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create a `Video` object, add it to the `Stage`, and stream a file in
    through a basic `NetConnection` and `NetStream` pair:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following classes into your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For this recipe, we must declare a number of different objects up front. We
    are, in this case, packaging a video file along with the application itself; we
    will declare a `String` constant referring to this file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next set of objects pertains to the actual video stream. Declare a `Video`
    object to display the `NetStream` data coming in over our local `NetConnection`.
    We will also declare an `Object` to bind specific, necessary functions to for
    video playback.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we will declare a `TextField` and `TextFormat` pair to relay text
    messages onto the device display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now set up our `TextField`, apply a `TextFormat`, and add it to the
    `DisplayList`. Here, we create a method to perform all of these actions for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now to set up our video connection; we will create a new Object called `streamClient`,
    which we will use to bind a number of helper functions to our stream objects.
    A `Video` object must be created and added to the `DisplayList` in order for the
    user to actually view the video stream. Finally, we create a `NetConnection`,
    assign `streamClient` to its `client` property, register an event listener to
    monitor connection status, and then invoke the `connect()` method, passing in
    `null` as the connection argument, since we are not using any sort of media server
    in this example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We may not always want to set the `Video.smoothing` property to true; in this
    case, since we are unsure exactly how large the video is, we will enable it in
    order to smooth any potential artifacting that may occur through scaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following method will be called from our `onNetStatus` function once we
    are sure the `NetConnection` has connected successfully. Within this method, create
    a new `NetStream` object to stream the video over our `NetConnection`. We will
    also assign `streamClient` to the `client` property and register an event listener
    to monitor stream status. To display the stream through our `Video` object, use
    the `attachStream()` method and pass in our `NetStream` object. Now, simply invoke
    the `play()` method, passing in our `videoPath` constant, and pointing to the
    video file location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `onNetStatus` method, as defined in the following code snippet, can be used
    with both our `NetStream` and `NetConnection` objects in order to make decisions
    based upon the different status messages returned. In this example, we are either
    firing the `connectStream` method once a `NetConnection` is successfully connected,
    or performing some scaling and layout once we are sure the `NetStream` is playing
    successfully.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a comprehensive list of all supported `NetStatusEvent` info codes, have
    a look at: [http://help.adobe.com/en_US/FlashPlatform/reference/actionscript/3/flash/events/NetStatusEvent.html#info](http://help.adobe.com/en_US/FlashPlatform/reference/actionscript/3/flash/events/NetStatusEvent.html#info).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next three steps include methods which have been bound to the client property
    of either the `NetConnection` or `NetStream`. These must exist as part of the
    client object, or else errors may be thrown as they are expected methods. The
    `onTextData` method fires whenever text is encountered within the file being streamed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `onMetaData` method fires when the stream metadata is loaded into the application.
    This provides us with many useful pieces of information, such as stream width,
    height, and duration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `onCuePoint` method fires whenever embedded cue points are encountered
    within the file being streamed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting application will look similar to the following screen render:![How
    to do it…](img/1420_05_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The entire workflow is almost exactly what would be used when developing for
    the desktop. When playing back video over Flash, we must first establish a `NetConnection`
    for our `NetStream` to travel across. Once the `NetConnection` is connected, we
    create our `NetStream` and bind the two of them together. Adding a `Video` object
    to the `Stage` will enable the stream to be viewable on our device, so long as
    we attach out `NetStream` to it. At this point, we can then play any files we
    wish over that `NetStream` by simply invoking the `play()` method.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with `NetConnection` and `NetStream`, there is always the need
    to create a number of helper functions. These functions include the registration
    of event listeners to detect particular status events, and the definition of a
    custom `client` property with associated methods that will be expected by the
    established workflow.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we are playing a file packaged with our application. It would
    be just as simple to play a video file from the device gallery (assuming the codec
    used to compress the video is supported by Flash and AIR) or progressively stream
    a video over HTTP from a location available over a wireless network connection.
  prefs: []
  type: TYPE_NORMAL
- en: The video file we are playing back through Flash player or AIR must be of a
    type which is supported by the Flash Platform runtimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid video file types include:'
  prefs: []
  type: TYPE_NORMAL
- en: FLV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MP4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: M4V
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F4V
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3GPP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flash Platform runtimes support every level and profile of the H.264 standard
    and retain full FLV support as well. However, recommended resolutions specific
    to Android are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**4:3 video:** 640 × 480, 512 × 384, 480 × 360'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**16:9 video:** 640 × 360, 512 x 288, 480 × 272'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When packaging such an application, which utilizes files that are distributed
    as part of the application package, we will also need to be sure and include them
    through the use of a GUI (if your IDE supports this) or as extra files in the
    command line compilation process.
  prefs: []
  type: TYPE_NORMAL
- en: Playing remote video streams over RTMP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aside from the playback of video available through the local file system or
    from a remote HTTP web address, we also have the ability to stream video files
    onto Android devices using Flash Media Server and the RTMP protocol. If a streaming
    server such as this is available, you can make great use of this when deploying
    video across mobile Android devices.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe involves the playback of a video file that has been deployed to
    a Flash Media Server. You can actually set up a developer version of FMS for free
    if you do not have access to a production server. To find out more information
    about streaming video over **Real Time Messaging Protocol** (**RTMP**), you can
    have a look at the resources available at: [http://www.adobe.com/products/flashmediaserver/](http://www.adobe.com/products/flashmediaserver/)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create a `Video` object, add it to the `Stage`, and stream a file in
    through a `NetConnection` and `NetStream` pair over RTMP:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following classes into your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For this recipe, we must declare a number of different objects up front. We
    are, in this case, using a Flash Media Server to perform a stream over RTMP; we
    will declare a `String` constant referring to the FMS application path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next set of objects pertains to the actual video stream. Declare a `Video`
    object to display the `NetStream` data coming in over our local `NetConnection`.
    We will also declare an `Object` to bind specific, necessary function to for video
    playback.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we will declare a `TextField` and `TextFormat` pair to relay text
    messages onto the device display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now set up our `TextField`, apply a `TextFormat`, and add it to the
    `DisplayList`. Here, we create a method to perform all of these actions for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now to set up our video connection; we will create a new Object called `streamClient`,
    which we will use to bind a number of helper functions to our stream objects.
    A `Video` object must be created and added to the `DisplayList` in order for the
    user to actually view the video stream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we create a `NetConnection`, assign `streamClient` to its `client`
    property, register an event listener to monitor connection status, and then invoke
    the `connect()` method, passing in the predefined `fmsPath` constant as the connection
    argument. This is because we must make a connection to this application instance
    on the Flash Media Server before proceeding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following method will be called from our `onNetStatus` function once we
    are sure the `NetConnection` has connected successfully. Within this method, create
    a new `NetStream` object to stream the video over our `NetConnection`. We will
    also assign `streamClient` to the `client` property and register an event listener
    to monitor stream status.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To display the stream through our `Video` object, use the `attachStream()` method
    and pass in our `NetStream` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, simply invoke the `play()` method, passing in a `String` identifying the
    particular stream or file to play over RTMP. You will notice that since we are
    using an H.264 based file format, we must prefix the stream name with `mp4:`.
    If streaming live or via FLV, the prefix is not necessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `onNetStatus` method, as defined in the following code snippet, can be
    used with both our `NetStream` and `NetConnection` objects in order to make decisions
    based upon the different status messages returned. In this example, we are either
    firing the `connectStream` method once a `NetConnection` is successfully connected,
    or performing some scaling and layout once we are sure the `NetStream` is playing
    successfully:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The next three steps include methods which have been bound to the client property
    of either the `NetConnection` or `NetStream`. These must exist as part of the
    client object, else errors may be thrown as they are expected methods. The `onBWDone`
    method is particular to files streamed over RTMP. It fires whenever the streaming
    server has completed an estimation of client bandwidth available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `onTextData` method fires whenever text is encountered within the file being
    streamed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `onMetaData` method fires when the stream metadata is loaded into the application.
    This provides us with many useful pieces of information, such as stream width,
    height, and duration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `onCuePoint` method fires whenever embedded cue points are encountered
    within the file being streamed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting application will look similar to the following screen render:![How
    to do it…](img/1420_05_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When playing back RTMP streams, we must first establish a `NetConnection` for
    our `NetStream` to travel across. The `NetConnection` will attempt to connect
    to the specified application defined on a Flash Media Server address. Once the
    `NetConnection` is connected, we create our `NetStream` and bind the two of them
    together. Adding a `Video` object to the `Stage` will enable the stream to be
    viewable on our device, as long as we attach out `NetStream` to it. At this point,
    we can then play any files we wish over that `NetStream` by simply invoking the
    `play()` method.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with `NetConnection` and `NetStream`, there is always the need
    to create a number of helper functions. These functions include the registration
    of event listeners to detect particular status events, and the definition of a
    custom `client` property with associated methods that will be expected by the
    established workflow.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we are streaming a video file through an RTMP location over
    the Internet through Flash Media Server. You can use this same technique to stream
    audio files over RTMP or write a video chat application using the device camera.
    While we demonstrate here how to generate a `Video` object from scratch, keep
    in mind that there are various component solutions available such as the `FLVPlayBack`
    control that ships with Flash Professional, and the `VideoDisplay` and `VideoPlayer`
    components, which are part of the Flex framework. There are endless possibilities
    with this technology!
  prefs: []
  type: TYPE_NORMAL
- en: Playing audio files from the local filesystem or over HTTP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The playback of audio files through Flash Platform runtimes on Android devices
    is fairly straightforward. We can point to files bundled with our application,
    as this recipe demonstrates, files on the device storage, or files over a remote
    network connection. No matter where the file is located, playback is accomplished
    in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We must load the audio file into a `Sound` object and will then have the ability
    to manipulate playback, volume, pan, among other properties. In this recipe, we
    will allow the user to control volume through the rotation of a basic dial:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following classes into your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For this recipe, we must declare a number of different objects up front. We
    will begin with a sound object group consisting of `Sound, SoundChannel`, and
    `SoundTransform`. These objects will allow us to take full control over the audio
    for this recipe. We will also create a `Sprite`, which will serve as a user interaction
    point. Finally, we will declare a `TextField` and `TextFormat` pair to relay text
    messages onto the device display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now set up our `TextField`, apply a `TextFormat`, and add it to the
    `DisplayList`. Here, we create a method to perform all of these actions for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To create our volume dial, we will initialize a new `Sprite` and use the `graphics`
    API to draw a representation of a dial within it. We then add this `Sprite` to
    the `Stage:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we will go about setting up our audio related objects. Initialize our `Sound`
    and load a `MP3` file into it through `URLRequest`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will set the initial volume of the sound to 50% by creating a `SoundTransform`
    and passing in a value of `0.5` as the `volume` in ActionScript is registered
    in a range of `0 -1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To play the `Sound`, we will create a `SoundChannel` object, assign our `SoundTransform`
    to its `soundTransform` property, and finally set the `SoundChannel` through the
    `Sound.Play()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the specific input mode for the multitouch APIs to support touch input
    by setting `Multitouch.inputMode` to the `MultitouchInputMode.GESTURE` constant.
    We will also register a listener for `TransformGestureEvent.GESTURE_ROTATE` events
    upon our `Sprite` to intercept user interaction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When the `Sprite` is rotated by a user, we want to adjust playback volume accordingly.
    To accomplish this, we will adjust the `Sprite rotation` based upon the data received
    from our gesture event. We can then convert the `Sprite rotation` into a valid
    `volume Number` and modify the `SoundTransform` to reflect this, which will raise
    or lower the volume of our audio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting application will look similar to the following screen render:![How
    to do it…](img/1420_05_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We load an audio file into a `Sound` object in ActionScript through a `URLRequest`
    to make it available to our application. Simple playback can be achieved by invoking
    the `play()` method upon the `Sound`, but we retain a greater amount of control
    by assigning the sound playback onto a `SoundChannel` object, as we can then control
    things aspects such as pan and volume through the construction and assignment
    of a `SoundTransform` object. In this recipe, we modify the volume of the `SoundTransform`
    and then assign it to the `SoundChannel.soundTransform` property upon which our
    `Sound` is playing, thus modifying the sound.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we are playing a file packaged with our application. It would
    be just as simple to play an audio file from the device file system (assuming
    the codec used to compress the audio is supported by Flash and AIR) or progressively
    stream a file over HTTP from a location available over a network connection.
  prefs: []
  type: TYPE_NORMAL
- en: The audio file we are playing back through Flash Player or AIR must be of a
    type that is supported by the Flash Platform runtimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid audio formats include:'
  prefs: []
  type: TYPE_NORMAL
- en: FLV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MP3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AAC+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HE-AAC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AAC v1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AAC v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When packaging such an application, which utilizes files which are distributed
    as part of the application package, we will also need to be sure and include them
    through the use of a GUI (if your IDE supports this) or as extra files in the
    command line compilation process.
  prefs: []
  type: TYPE_NORMAL
- en: Generating an audio spectrum visualizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to generate some sort of visual feedback when playing audio is very
    useful to the user, as they will be able to see that playback occurs even if the
    device volume has been muted or turned down. Generating visuals from audio is
    also useful in certain games, or in monitoring audio input levels.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load a `MP3` file into a `Sound` object. By employing the `SoundMixer.
    computeSpectrum()` method, we can access the actual bytes being played back and
    construct visualizations with this data using the `Sprite graphics` API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following classes into your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For this recipe, we must declare a number of different objects up front. We
    will begin with a sound object pair consisting of `Sound` and `SoundChannel`.
    These objects will allow us to take full control over the audio for this recipe.
    We will also create a `Sprite`, which will serve as a canvas to draw out audio
    spectrum data. Finally, we will declare a `Timer` in order to refresh the sound
    spectrum visualization every few milliseconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To construct the canvas within which we will draw out visualization elements,
    we must initialize a `Sprite`, define a particular line style on the `graphics`
    API, and add it to the `Stage:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A `Timer` will be used to determine how often we will refresh the visualization
    within our container `Sprite`. In this case, we will set it to fire a `TIMER`
    event every 100 milliseconds, or 10 times every second.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we will go about setting up our audio related objects. Initialize our `Sound`
    and load a `MP3` file into it through `URLRequest`. To play the `Sound`, we will
    create a `SoundChannel` object, assign our `SoundTransform` to its `soundTransForm`
    property, and finally set the `SoundChannel` through the `Sound.Play()` method.
    As we now have our `Sound` loaded and ready to go, we can start running our `Timer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, construct a method similar to the following, which will extract byte
    data from the global Flash `SoundMixer`, and use the `graphics` API to draw out
    visualizations based upon this data. We first initialize a number of variables
    to be used in this method and run `computeSpectrum()` off of the `SoundMixer`
    class. This will populate our `ByteArray` with all of the sound sample data needed
    to create our visuals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In looping through the data, we can use the `graphics` API to draw lines, circles,
    or anything we desire into our `Sprite` container. In this case, we draw a series
    of lines to create a spectrum visualization. As this is set to update every 100
    milliseconds, it becomes an ever-shifting visual indicator of the sound being
    played back.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting application will look similar to the following screen render:![How
    to do it…](img/1420_05_09.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `SoundMixer` class provides access to the `computeSpectrum()` method, which
    is able to take a snapshot of the any sound being played through Flash Player
    or AIR and write it into a `ByteArray` object. There are 512 total `Number` values
    written to the `ByteArray`; the first 256 represent the left channel, and the
    remaining 256 represent the right. Depending upon what sort of visualization you
    need, the full 512 values may not be needed, as in the case here.
  prefs: []
  type: TYPE_NORMAL
- en: To generate the values which determine where to draw our lines using the graphics
    API, we use `ByteArray.readFloat()`, which reads a 32-bit floating-point value
    from the byte stream, and converts it to a `Number`. As this value indicates the
    specific sound data for that particular sample, we can use that to draw out a
    series of lines through the graphics API and form our visible spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find a large amount of additional methods and formulae online by doing
    a simple search. The possibilities for doing this sort of generative visualization
    are truly endless, but we must take into account the lower than normal hardware
    specifications on these devices when deciding how far to push any visualization
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: Generating audio tones for your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Packing a lot of sound files into an application is one method of including
    audio. Another method is the runtime generation of sound data. We'll produce some
    simple sine tones in this recipe, which vary based upon detected touch pressure.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will examine how to generate audio sample byte data based upon user touch
    pressure and feed this into a `Sound` object to generate a variety of tones:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following classes into your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For this recipe, we must declare a number of different objects up front. We
    will begin with a sound object pair consisting of `Sound` and `SoundChannel`.
    These objects will allow us full control over the audio for this recipe. We will
    also create a `Number`, which will retain pressure information obtained through
    user touch. Finally, we will declare a `TextField` and `TextFormat` pair to relay
    text messages onto the device display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now set up our `TextField`, apply a `TextFormat`, and add it to the
    `DisplayList`. Here, we create a method to perform all of these actions for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will go about setting up our audio related objects. Initialize a `Sound`
    and `SoundChannel` object pair. These will be employed later on to play back our
    generated audio data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set the specific input mode for the multitouch APIs to support touch input by
    setting `Multitouch.inputMode` to the `MultitouchInputMode.TOUCH_POINT` constant.
    We will also register a listener for `SampleDataEvent.SAMPLE_DATA` events, which
    requests will begin once we set out `Sound` object to `play()` through the previously
    established `SoundChannel:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Whenever a touch event is detected, we will monitor it through the following
    method. Basically, we modify the `touchPressure Number`, which will be used to
    calculate our sine wave generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our final method will execute whenever the currently playing `Sound` object
    requests new sample data to play back. We will employ the `ByteArray.writeFloat()`
    method to send generated audio data back to our `Sound` object for playback upon
    each sample request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting application will produce a variable tone depending upon the amount
    of pressure applied through touch and should look similar to the following screen
    render:![How to do it…](img/1420_05_10.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ActionScript `Sound` object, when registered with a `SampleDataEvent` event
    listener, will act as a socket when playback is initiated. We must provide sample
    data to pass along to this `Sound` object through a function, which generates
    this data, and passes samples to the waiting `Sound` object. The number of samples
    can vary between 2048 and 8192, in this case, we provide as much sample data as
    possible. The general formula provided by Adobe for generating a sine wave is:
    `Math.sin((Number(loopIndex+SampleDataEvent.position)/Math.PI/2))` multiplied
    by 0.25\. Since we are modifying the formula based upon recorded touch point pressure,
    we multiply by this recorded value, instead. This modifies the generated audio
    that is produced by the application.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a more controlled library of generated sound tones, there exist ActionScript
    libraries, which can be used free of charge, or for a fee, depending on the library.
    I'd recommend checking out Sonoport at [http://www.sonoport.com/](http://www.sonoport.com/).
  prefs: []
  type: TYPE_NORMAL
