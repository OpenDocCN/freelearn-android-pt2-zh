# 第九章。收集和存储数据

我们继续！在前一章中，我们介绍了几个流行的外部数据库，您可以使用它们，并决定使用谷歌的应用引擎(GAE)开发一个功能齐全的后端。我们设法在 GAE 上创建了一个新项目，并使用 `PersistenceManager`构建了一个极其有用的包装类，它展示了我们的 JDO 数据库的一些核心概念。当我们开始插入真实数据并随后使用我们的安卓应用程序查询该数据时，这个包装类很快就会非常方便。

我们到了——下一步！对于大多数试图构建以数据为中心的应用程序的人来说，实际获取这些数据将非常困难，并且通常需要大量的时间和金钱。然而，我们有许多工具和方法可以帮助我们使用现有的数据来填充我们的数据库。在下一章中，我们将看看其中的一些方法，最后将把我们新获得的数据插入到我们的 JDO 数据库中。

# 收集数据的方法

首先，让我们简单介绍两种不同的数据收集方式:

*   通过应用编程接口
*   通过刮网

第一种也是最简单的方法是使用应用编程接口。对于那些以前从未使用过应用编程接口的人来说，可以把它想象成由某个第三方公司创建的*网络库*，它通常允许你调用一些函数(几乎总是作为 HTTP 请求执行)，然后让你访问它们数据的子集。

例如，一个常见的应用编程接口是脸书图形应用编程接口，当通过身份验证时，它允许您查询用户的配置文件信息或事件的详细信息，等等。本质上，通过这个应用编程接口，我可以通过一个不同的渠道，访问与我在脸书网站上看到的人或事件相同的数据。这就是我所说的公司*暴露*他们数据的子集。另一个例子可能是 Yelp，它的 API 允许您在传递一组参数(即位置)时查询餐馆和场地。在这里，即使我实际上不在 Yelp 的网页上，我仍然可以通过他们的 API 访问他们的数据。

拥有一个可用于收集数据的应用编程接口是非常有用的，因为数据已经存在并准备好供您使用；根据公司的可信度，数据通常已经被清理并格式化。这使您不必自己查找数据，然后自己清理数据。然而，问题是，通常情况下，由于专有原因，公司不允许您存储他们的数据，因此，根据您的应用程序所做的工作，您可能需要记住这个法律问题。

那么，当没有可供您使用的应用编程接口时，到底会发生什么呢？那么，你将不得不求助于自己获取数据，一个很好的方法是通过**网页抓取**。在下一节中，我将花大量的时间来解释什么是网页抓取的艺术，以及如何去做。现在，让我们以讨论 API 通常返回数据的两种流行格式来结束这一小段。

第一种称为可扩展标记语言(XML)，是一种人类可读和机器可读的数据格式，它采用树的形式，实际上看起来非常类似于 HTML。这个树形结构的一个简单例子是，你调用脸书图形应用编程接口，它返回你的朋友列表。树根可能有标签`<friends>`，在树根下面可能有一系列带有标签`<friend>`的叶子。然后，每个`<friend>`节点可能分支成几个描述符标签，例如`<name>, <age>`，等等。事实上，在后面的例子中，我实际上将使用 XML 作为选择的数据格式，因为它是人类可读的，所以您将看到这种情况的真实例子。

下一个叫做 **JavaScript 对象符号(JSON)** ，它是一个比 XML 更轻量级的数据结构。JSON 仍然是机器可读的，但是对于人类可读性来说不太友好。不过，权衡的结果是解析 JSON 往往会更高效，因此实际上，使用哪一个取决于人类可读性相对于性能有多重要。JSON 的一般结构类似于地图而不是树。使用前面相同的例子，不是返回一个以`<friends>`为根节点的树结构，而是将 `friends`作为一个值等于 JSON 数组的键。JSON 数组将有一个 `friend`键的列表，每个键都有一个等于 JSON 对象的值。最后，JSON 对象将具有等于 `name, age`的键，依此类推。换句话说，你可以把 JSON 结构想象成一系列嵌入的映射，其中很多时候键会指向一个子映射，然后子映射有自己的键等等。

因此，在使用第三方 API 时，您通常需要知道他们选择哪种数据格式来返回数据，并相应地解析结果。此外，即使当您正在实现网页抓取器并发现自己必须构建自己的应用编程接口时，选择两种数据格式中的一种并坚持下去通常会有所帮助。当从外部应用程序调用自己的应用编程接口，然后解析返回的结果时，这将使您的生活变得更加简单。现在，转到网页抓取。

# 刮网底漆

网页抓取是构建网页 HTML 并有条不紊地从中解析数据的艺术。这个想法是，HTML 应该(在某种程度上)内在地具有良好的结构，因为每个开放标签(也就是说，`<font>)`后面应该跟一个封闭标签(也就是说，`</font>)`)。这样，HTML 如果结构正确的话，可以看作是一个非常像 XML 的树形结构。抓取一个网站可以通过多种方式来实现，这些方式通常会随着底层 HTML 源代码的复杂程度而变化，但在高层次上，它涉及三个步骤:

1.  获取所需的网址，建立到该网址的连接，并检索其源代码。
2.  结构并清理底层源代码，使其成为有效的 XML 文档。
3.  运行像 XPath(或 XQuery 和 XSLT)这样的树导航语言，和/或使用正则表达式(REGEX)来解析出所需的节点。

第一步相对不言自明，但我会注意一件事。通常，你会发现自己需要抓取某种动态网页，这意味着网址不会是静态的，可能会根据日期、一些标准等发生变化。让我们通过两个例子来说明我的意思。

第一个涉及股票。假设你正在尝试编写一个网页抓取器，可以抓取给定股票的当前价格，比如雅虎！金融。首先，网址是什么样子的？快速查看谷歌当前的价格，我们看到相应网页的网址是:

[http://finance.yahoo.com/q?s=GOOG](http://finance.yahoo.com/q?s=GOOG)

这是一个非常简单的网址，我们会很快注意到股票的股票代码作为一个参数传递给了这个网址。在这种情况下，参数具有键 `s`和值等于跑马灯。现在很容易看出我们如何快速构建一个动态 URL 来解决我们的问题——我们所要做的就是编写一个简单的方法，如下所示:

```java
public void stockScraper(String ticker) {
String URL_BASE = "http://finance.yahoo.com/q?s=";
String STOCK_URL = URL_BASE + ticker;
// CONTINUE SCRAPING STOCK_URL
}

```

很整洁，对吧？现在假设我们不仅仅想要当前的股票价格，而是想要拉两个日期之间的所有历史价格。好吧，首先让我们看看一个样本网址是什么，同样是谷歌的股票:

[http://finance.yahoo.com/q/hp?s=GOOG&a = 07&b = 19&c = 2004&d = 02&e = 14&f = 2012](http://finance.yahoo.com/q/hp?s=GOOG&a=07&b=19&c=2004&d=02&e=14&f=2012)

我们在这里注意到了什么？我们注意到跑马灯仍然作为参数通过键 `s`传递，但是除此之外，我们注意到两个不同的日期通过不同的键传递。日期看起来像 07/19/2004，很可能是开始日期，02/14/2012，似乎是结束日期，它们似乎有关键值 `a`到 `f`。在这种情况下，关键值并不是最直观的，通常你会看到 `day`或 `d`以及 `month`或 `m`的关键值。然而，想法很简单，有了这个网址，你不仅可以动态调整滚动条是什么，而且根据你的用户正在寻找的日期范围，你也可以调整它们。通过记住这个想法，你将慢慢学会如何更好地破译各种网址，并学会如何使它们极其动态，并适合你的刮擦需求。

现在，一些网站通过 POST 请求来提出请求。不同的是，在 POST 请求中，参数嵌入在请求中(而不是嵌入在 URL 中)。这样，潜在的私有数据就不会显示在 URL 中(尽管这只是 POST 请求的一个用例)。在这种情况下，我们该怎么办？嗯，没有特别简单的答案。通常，您需要下载一个 HTTP 请求监听器(对于像 Chrome 和 Firefox 这样的浏览器，只需搜索一个 HTTP 请求监听器插件)。这将允许您查看正在进行哪些请求(GET 和 POST 请求)，以及传递的参数。一旦知道了参数是什么，剩下的就像 GET 请求一样工作了。

现在，一旦我们有了我们的网址，下一步就是获取底层源代码并构建它。当然，这对于你自己来说可能是一件痛苦的事情，但是幸运的是，有一些库可以为我们清理和构建源代码。我最常用的一个叫做 **HtmlCleaner** ，可以在以下网址找到:

[http://htmlcleaner.sourceforge.net/](http://htmlcleaner.sourceforge.net/)

这是一个很棒的库，它为您提供了清理和构造源代码、导航生成的 XML 文档以及最终解析 XML 节点的值和属性的方法。一旦我们的数据被清理，最后一步就是简单地遍历树，挑选出我们想要的数据。现在，这说起来容易做起来难，因为仅仅使用 Java 及其原生包并没有真正简单的方法来有条不紊地、可靠地遍历树。我所说的有条不紊和可靠是指即使底层源代码的结构略有变化，也能够遍历树并解析正确的数据。

例如，假设你的解析方法像告诉你的代码给你第五个节点的值一样幼稚。那么，当雅虎！(或者你正在抓取的任何网站)决定给他们的网站添加一个新的标题，现在第五个节点变成了第六个？即使在对基础进行相对简单的更改的情况下，您的刮板也会断裂，并开始从不正确的节点向您返回值，因此理想情况下，我们希望找到一种方法来获得正确的节点值，而不管基础网站如何更改。

对我们来说幸运的是，前端工程师经常会构建网站，其中重要字段会有包含具有唯一值的 `class`或 `id`属性的标签。然后我们可以利用这些有用的和描述性的命名约定，并使用一种叫做 **XPath** 的漂亮语言。一旦你看到语言本身，它是相当不言自明的；事实上，语法类似于任何路径(即目录路径、URL 路径等)，所以如果您愿意，我将简单地引导您到下面的 URL 来了解前因后果:

[http://www.w3schools.com/xpath/](http://www.w3schools.com/xpath/)

无论如何，现在请记住，XPath 是一种简单的语言，它允许您返回由路径决定的节点集。XPath 的特别之处在于，在路径中，您可以通过包含各种过滤器来进一步细化您的搜索，例如，允许我们只返回属于某个 `class`的那些 `div`。这就是为什么拥有描述性的 `class`和 `id`属性会派上用场，因为我们可以深入到 HTML 中，并且只有效地找到那些对我们重要的节点。此外，如果您仍然需要额外的工具来解析生成的 XML，您可以包含正则表达式(REGEX)来帮助您进行搜索。

最后，我们的想法是尽可能健壮地进行解析，因为您最不想做的事情就是不断更新您的抓取器，因为对基础网站进行了微小的、无关紧要的更改。同样，有时网站会发生巨大变化，你不得不合理地更新你的刮刀，但想法是尽可能健壮地编写它们。

在这一点上，我相信你有很多问题。代码实际上是什么样子的？如何抓取网站的 HTML？你是如何使用 `HtmlCleaner`图书馆的？XPath 的一个例子是什么？之前，我的目标是让大家对什么是网页抓取有一个高层次的理解，在这个过程中，我引入了许多不同的技术和人们会使用的技巧。现在，让我们用一些代码来弄脏我们的手，看看前面的每个步骤是如何操作的。以下是抓取重磅视频游戏数据的第一步和第二步:

```java
public class HTMLNavigator {
// STEP 1 - GET THE URL'S SOURCE CODE
public static CharSequence navigateAndGetContents(String url_str) throws IOException {
URL url = new URL(url_str);
// ESTABLISH CONNECTION TO URL
URLConnection conn = url.openConnection();
conn.setConnectTimeout(30000);
String encoding = conn.getContentEncoding();
if (encoding == null) {
encoding = "ISO-8859-1";
}
// WRAP BUFFERED READER AROUND INPUT STREAM
BufferedReader br = new BufferedReader (new InputStreamReader(conn.getInputStream(), encoding));
StringBuilder sb = new StringBuilder();
try {
String line;
while ((line = br.readLine()) != null) {
sb.append(line);
sb.append('\n');
}
} finally {
br.close();
}
return sb;
}
}

```

首先，我们有一个简单的便利类，它允许我们获取传入网址的源代码。它只是打开一个连接，设置一些标准的 web 参数，然后读取输入流。我们使用一个 `StringBuilder`来高效地构造一个包含输入流每一行的大字符串，最后关闭所有连接并返回该字符串。该字符串将成为传入网址的底层 HTML，也是我们在下一步构建一个干净的、有组织的 XML 文档所需要的。其代码如下:

```java
import org.htmlcleaner.CleanerProperties;
import org.htmlcleaner.HtmlCleaner;
import org.htmlcleaner.TagNode;
import org.htmlcleaner.XPatherException;
import app.helpers.HTMLNavigator;
import app.types.VideoGame;
public class VideoGameScraper {
private static String content;
private static final String BASE_URL = "http://www.blockbuster.com/
games/platforms/gamePlatform";
/**
* QUERY FOR GAMES OF CERTAIN PLATFORM
*
* @param type
* the platform type
* @return
* @throws IOException
* @throws XPatherException
*/
public static List<VideoGame> getVideoGamesByConsole(String type) throws IOException, XPatherException {
// CONSTRUCT FULL URL
String query = BASE_URL + type;
// STEPS 1 + 2 - GET AND CLEAN THE DYNAMIC URL
TagNode node = getAndCleanHTML(query);
// STEP 3 - PARSE AND ADD GAMES
List<VideoGame> games = new ArrayList<VideoGame>();
. . .
return games;
}
/**
* CLEAN AND STRUCTURE THE PASSED IN HTML
*
* @param result
* the underlying html
* @return
* @throws IOException
*/
private static TagNode getAndCleanHTML(String result) throws IOException {
String content = HTMLNavigator.navigateAndGetContents(result). toString();
VideoGameScraper.content = content;
// USE HTMLCLEANER TO STRUCTURE HTML
HtmlCleaner cleaner = new HtmlCleaner();
CleanerProperties props = cleaner.getProperties();
props.setOmitDoctypeDeclaration(true);
return cleaner.clean(content);
}
.
.
.
}

```

因此，在这里，我们首先编写一个简单的方法，它允许我们连接到结果网址并获取其底层源代码。然后，我们将该结果传递给一个清理方法，该方法实例化我们的 `HtmlCleaner`类的一个新实例，并调用 `clean()`方法。这个方法将把底层的 HTML 构造成一个格式良好的 XML 文档，并将 XML 的根作为一个 `TagNode`对象返回。最后一步是简单地查看底层源代码，确定什么是正确的 XPathss，然后在给定的根 `TagNode`上运行这些 XPath。百视达视频游戏租赁页面的简略源代码看起来像下面的代码:

```java
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en" xml:lang="en" >
<head>
<body class="full">
<script type="text/javascript">
<div class="body clearDiv">
<div id="pageMask">&nbsp;</div>
<div id="boxPopup">&nbsp;</div>
<div id="head" class="head">
<style type="text/css">
<div>
<div id="gamesNav" class="secondaryNav">
<script type="text/javascript" language="javascript">
<div class="page clearDiv">
<div class="main contentsMain clearDiv">
<div class="primary clearDiv">
<span class="contentsDM"></span>
<span class="contentsLB"></span>
<img align="right" src="/content/v.5.134.3.20120309142008/img/games/platforms/gameXboxOrig.gif" alt="Xbox Games">
<h1>Action &amp; Adventure Video Games</h1>
<div class="pagination">
<div class="gb6 listViewHeader">
<div class="">
<div id="4453" class="addToQueueEligible game sizeb gb6 bvr-gamelistitem ">
<mkt marketingitemid="4453" catalystinfo="A" listname="gameActionAdventure"></mkt>
<a onmouseover="if(DndUtil.windowLoaded){ new GameRollover(this); }" href="/games/catalog/gameDetails/4136" title="Superman Returns: The Video Game">
<img class="box" height="143" width="100" src="http://images. blockbuster.com/is/amg/games/cov200/drg200/g256/g25653wauzo. jpg?wid=100&hei=143">
</a>
<div class="details">
<h4>
<a onmouseover="if(DndUtil.windowLoaded){new GameRollover(this);}" href="/games/catalog/gameDetails/4136" title="Superman Returns: The Video Game">Superman Returns: The Video Game</a>
</h4>
<dl class="release">
<dl class="rated">
<div class="platform">
<dl class="movieInfo">
<div class="summary ">
<p class="readMore">
<div class="rolloverDetailsDiv" contentsrc="/esi/catalog/gameRollover/4136/false">&nbsp;</div>
</div>
<div class="movieOptions">
<div id="movieRating" class="ratingWidget">
</div>
</div>
...

```

然而，请注意，这个源代码是截止到我写这篇文章的日期，并不保证保持不变。但是，从上面这个源代码中，我们可以看到每一个游戏都被列在一个带有类 `addToQueueEligible game sizeb gb6 bvr-gamelistitem`的 `div`标签中。这是一个有点长的类名，但是我们可以有一些信心，通过搜索带有这个类标签的 `divs`，我们将找到视频游戏，并且仅仅是视频游戏，因为类标签涉及如何将合格的游戏添加到队列中。

现在，一旦我们到达那些期望的 `divs`，我们看到我们想要的节点仅仅是第一个 `a`节点，以及那个 `a`节点对应的 `img`标签。因此，为了分别获得标题和图片网址，我们想要的 XPaths 应该如下所示:

```java
//div[@class='addToQueueEligible game sizeb gb6 brv-gamelistitem']/a[1]
//div[@class='addToQueueEligible game sizeb gb6 brv-gamelistitem']/a[1]/img

```

接下来，让我们来看看我们的刮刀的完整代码:

```java
import org.htmlcleaner.CleanerProperties;
import org.htmlcleaner.HtmlCleaner;
import org.htmlcleaner.TagNode;
import org.htmlcleaner.XPatherException;
import app.helpers.HTMLNavigator;
import app.types.VideoGame;
public class VideoGameScraper {
private static String content;
// XPATH FOR GETTING TITLE NAMES
private static String TITLE_EXPR = "//div[@class='%s']/a[1]";
// XPATH FOR GETTING IMAGE URLS
private static String IMG_EXPR = "//div[@class='%s']/a[1]/img";
// BASE OF BLOCKBUSTER URL
public static final String BASE_URL = "http://www.blockbuster.com/ games/platforms/gamePlatform";
/**
* QUERY FOR GAMES OF CERTAIN PLATFORM
*
* @param type
* the platform type
* @return
* @throws IOException
* @throws XPatherException
*/
public static List<VideoGame> getVideoGamesByConsole(String type) throws IOException, XPatherException {
// CONSTRUCT FULL URL
String query = BASE_URL + type;
// USE HTMLCLEANER TO STRUCTURE HTML
TagNode node = getAndCleanHTML(query);
// ADD GAMES
List<VideoGame> games = new ArrayList<VideoGame>();
games.addAll(grabGamesWithTag(node, "addToQueueEligible game sizeb gb6 bvr-gamelistitem ", type));
return games;
}
/**
* GIVEN THE STRUCTURED HTML, PARSE OUT NODES OF THE PASSED IN TAG
*
* @param head
* the head of the structured html
* @param tag
* the tag we are looking for
* @param type
* the platform type
* @return
* @throws XPatherException
*/
private static List<VideoGame> grabGamesWithTag(TagNode head, String tag, String type) throws XPatherException {
// RUN VIDEO GAME TITLE AND IMAGE XPATHS
Object[] gameTitleNodes = head.evaluateXPath(String.format (TITLE_EXPR, tag));
Object[] imgUrlNodes = head.evaluateXPath(String.format (IMG_EXPR, tag));
// ITERATE THROUGH VIDEO GAMES
List<VideoGame> games = new ArrayList<VideoGame>();
for (int i = 0; i < gameTitleNodes.length; i++) {
TagNode gameTitleNode = (TagNode) gameTitleNodes[i];
TagNode imgUrlNode = (TagNode) imgUrlNodes[i];
// BY LOOKING AT THE HTML, WE CAN DETERMINE
// WHICH ATTRIBUTES OF THE NODE TO PULL
String title = gameTitleNode.getAttributeByName("title");
String imgUrl = imgUrlNode.getAttributeByName("src");
// BUILD OUR VIDEO GAME OBJECT AND ADD TO LIST
VideoGame v = new VideoGame(title, imgUrl, type);
games.add(v);
}
return games;
}
/**
* CLEAN AND STRUCTURE THE PASSED IN HTML
*
* @param result
* the underlying html
* @return
* @throws IOException
*/
private static TagNode getAndCleanHTML(String result) throws IOException {
. . .
}
}

```

就这样！我们之前已经看到了大部分代码，所以实际上这只是我们应该钻研的 `grabGamesWithTag()`方法。该方法的第一部分是采用我们之前看到的 HTML 模式(在网站的源代码中)，并将它们与我们的 XPath 格式相结合。在这一点上，我们有一个有效的 XPath，它将引导我们找到视频游戏的标题，以及视频游戏的图像网址。我们需要使用来自 `HtmlCleaner`的方法来实际运行这个 XPath 命令，如下所示:

```java
Object[] gameTitleNodes = head.evaluateXPath(String.format (TITLE_EXPR, tag));

```

这将返回一个 `Objects`列表，然后可以将其投射到单个 `TagNode`对象上。然后我们需要做的是循环遍历数组中的每个 `Object`，将其转换为 `TagNode`，并提取节点的值或节点的属性来获得所需的数据。我们可以在方法的以下部分看到:

```java
// ITERATE THROUGH VIDEO GAMES
List<VideoGame> games = new ArrayList<VideoGame>();
for (int i = 0; i < gameTitleNodes.length; i++) {
TagNode gameTitleNode = (TagNode) gameTitleNodes[i];
TagNode imgUrlNode = (TagNode) imgUrlNodes[i];
// BY LOOKING AT THE HTML, WE CAN DETERMINE
// WHICH ATTRIBUTES OF THE NODE TO PULL
String title = gameTitleNode.getAttributeByName("title");
String imgUrl = imgUrlNode.getAttributeByName("src");
// BUILD OUR VIDEO GAME OBJECT AND ADD TO LIST
VideoGame v = new VideoGame(title, imgUrl, type);
games.add(v);
}

```

在这两种情况下，我们需要的值是节点的特定属性，而不是节点的值。如果它是一个值，我们的代码看起来会更像下面这样:

```java
List<VideoGame> games = new ArrayList<VideoGame>();
for (int i = 0; i < gameTitleNodes.length; i++) {
TagNode gameTitleNode = (TagNode) gameTitleNodes[i];
TagNode imgUrlNode = (TagNode) imgUrlNodes[i];
String title = gameTitleNode.getText().toString();
String imgUrl = imgUrlNode.getAttributeByName("src");
// BUILD OUR VIDEO GAME OBJECT AND ADD TO LIST
VideoGame v = new VideoGame(title, imgUrl, type);
games.add(v);
}

```

在这一点上，我们已经运行了一个快速入门网刮。同样，网页抓取是一种技术和艺术，需要时间来适应和掌握，但这是一项很好的技能，将为在整个网络上挖掘数据带来无数机会。现在，关注本章中介绍的概念，而不是实际的代码。我这么说的原因是因为你的代码看起来如何在很大程度上取决于你试图抓取的网页。不会改变的是刮擦背后的概念，所以用这一章提到的三个步骤来指导你如何为任何网页编写一个刮擦器。

# 为 GET/POST 方法扩展 HTTP servlets

现在我们已经编写了我们的网页抓取器，我们需要一种方法来获取返回的 `VideoGame`对象，并将它们实际存储在我们的数据库中。此外，一旦我们的服务器启动并运行，我们需要一种方式与它通信，并告诉它刮擦该网站，并将其插入我们的 JDO 数据库。我们与服务器通信的网关是通过一个叫做 HTTP servlet 的东西——我们在本书前面简单提到过。

以这种方式设置您的后端将特别有用，当我们稍后讨论 CRON 作业时，为了自动运行某种功能，需要一个 servlet 与之通信(稍后将详细介绍)。不过现在，让我们看看如何扩展 `HttpServlet`类并实现其 `doGet()`方法，该方法将侦听并处理发送给它的所有 HTTP GET 请求。但是首先，HTTP GET 请求到底是什么？好吧，HTTP web 请求只是用户向某个服务器发出请求，该请求将通过网络(即互联网)发送。根据请求的类型，服务器将处理并向用户发回一个 HTTP 响应。然后有两种常见类型的 HTTP 请求:

*   获取请求—仅用于检索数据的网络请求。这些 web 请求通常会要求服务器查询要返回的某种数据。
*   POST 请求—提交要处理的数据的 web 请求。通常，这将要求服务器插入用户提交的某种数据。

在这种情况下，由于我们没有为用户获取任何数据，也没有从用户提交任何数据(事实上，我们根本没有真正与任何用户进行交互)，因此我们使用哪种类型的请求并没有什么区别，因此我们将坚持使用更简单的 GET 请求，如下所示:

```java
import javax.servlet.ServletException;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
// EXTEND THE HTTPSERVLET CLASS TO MAKE THIS METHOD AVAILABLE
// TO EXTERNAL WEB REQUESTS, NAMELY CLIENTS AND CRON JOBS
public class VideoGameScrapeServlet extends HttpServlet {
private ArrayList<VideoGame> games;
/**
* METHOD THAT IS HIT WHEN HTTP GET REQUEST IS MADE
*
* @param request
* a servlet request object (any params passed can be retrieved
* with this)
* @param response
* a servlet response that you can embed data back to user
* @throws IOException
* @throws ServletException
*/
public void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException {
games = new ArrayList<VideoGame>();
String message = "Success";
try {
// GRAB GAMES FROM ALL PLATFORMS
games.addAll(
VideoGameScraper.getVideoGamesByConsole(VideoGameConsole.DS));
games.addAll(
VideoGameScraper.getVideoGamesByConsole(VideoGameConsole.PS2));
games.addAll(
VideoGameScraper.getVideoGamesByConsole(VideoGameConsole.PS3));
games.addAll(
VideoGameScraper.getVideoGamesByConsole(VideoGameConsole.PSP));
games.addAll(
VideoGameScraper.getVideoGamesByConsole(VideoGameConsole.WII));
games.addAll(
VideoGameScraper.getVideoGamesByConsole(VideoGameConsole.XBOX));
} catch (Exception e) {
e.printStackTrace();
message = "Failed";
}
// HERE WE ADD ALL GAMES TO OUR VIDEOGAME JDO WRAPPER
VideoGameJDOWrapper.batchInsertGames(games);
// WRITE A RESPONSE BACK TO ORIGINAL HTTP REQUESTER
response.setContentType("text/html");
response.setHeader("Cache-Control", "no-cache");
response.getWriter().write(message);
}
}

```

所以方法本身相当简单。我们早些时候已经有了 `getVideoGamesByConsole()`方法，它去做所有的刮擦，结果返回一个 `VideoGame`对象的列表。然后我们简单地为我们想要的每一个控制台运行它，最后使用我们漂亮的 JDO 数据库包装类并调用它的 `batchInsertGames()`方法来更快地插入。完成后，我们获取传入的 HTTP 响应对象，并快速向用户写回某种消息，让他们知道抓取是否成功。在这种情况下，我们不使用传入的 `HttpServletRequest`对象，但是如果请求者将参数传递到网址中，该对象将非常有用。例如，假设你想用一种只抓取一个特定游戏平台而不是所有游戏平台的方式来编写你的 servlet。在这种情况下，您需要某种方式将 p latform 类型的参数传递给 servlet，然后在 servlet 中提取传入的参数值。就像我们之前看到的雅虎一样！金融允许你输入具有关键价值的出票人 `s`，要输入平台类型，我们可以简单地做以下操作:

```java
http://{your-GAE-base-url}.appspot.com/videoGameScrapeServlet?type =Xbox

```

然后，在 servlet 端做:

```java
public void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException {
String type = request.getParameter("type");
games = new ArrayList<VideoGame>();
String message = "Success";
try {
// GRAB GAMES FROM SPECIFIC PLATFORM
games.addAll(VideoGameScraper.getVideoGamesByConsole(type));
} catch (Exception e) {
e.printStackTrace();
message = "Failed";
}
// ADD GAMES TO JDO DATABASE
VideoGameJDOWrapper.batchInsertGames(games);
// WRITE A RESPONSE BACK TO ORIGINAL HTTP REQUESTER
response.setContentType("text/html");
response.setHeader("Cache-Control", "no-cache");
response.getWriter().write(message);
}

```

很简单，对吧？您只需要确保 URL 中使用的键与您在 servlet 类中请求的参数相匹配。现在，将所有这些连接在一起的最后一步是在您的 GAE 项目中定义 URL 路径——也就是说，确保您的 GAE 项目知道 URL 模式实际上指向您刚刚编写的这个类。这可以在你的 GAE 项目的 `/war/WEB-INF/`目录中找到，特别是在 `web.xml`文件中。在这里，您需要添加以下内容。确保 servlet 名称和类路径匹配给定的 URL 模式:

```java
<?xml version="1.0" encoding="utf-8"?>
<web-app  version="2.5">
<servlet>
<servlet-name>videoGameScrapeServlet</servlet-name>
<servlet-class>app.httpservlets.VideoGameScrapeServlet</servlet-class>
</servlet>
<servlet-mapping>
<servlet-name>videoGameScrapeServlet</servlet-name>
<url-pattern>/videoGameScrapeServlet</url-pattern>
</servlet-mapping>
</web-app>

```

在这一点上，我们有了我们的刮刀，我们有了我们的 JDO 数据库，我们甚至有了我们的第一个 servlet，所有这些都连接好了，准备好了。最后一部分是安排你的铲运机定期运行；这样，您的数据库就有了最新的数据，而不必每天坐在电脑前手动调用刮刀。在下一节中，我们将看到如何使用 CRON 作业来实现这一点。

# 调度 CRON 作业

首先，让我们定义什么是 CRON 工作。术语 **cron** 最初指的是 Unix 中基于时间的作业调度程序，它允许您调度在特定时间定期运行的作业/脚本。同样的概念可以应用于 web 请求，在我们的例子中，目标是运行我们的 web 刮刀，并定期更新数据库中的数据，而不受我们的干扰。GAE 如此方便使用的另一个原因是该平台使调度 CRON 作业变得非常容易。为此，我们只需要在 GAE 项目的 `/war/WEB-INF/`目录中创建一个 `cron.xml`文件。在这个 XML 文件中，我们添加了以下代码:

```java
<?xml version="1.0" encoding="UTF-8"?>
<cronentries>
<cron>
<url>/videoGameScrapeServlet</url>
<description>Scrape video games from Blockbuster</description>
<schedule>every day 00:50</schedule>
<timezone>America/Los_Angeles</timezone>
</cron>
</cronentries>

```

这是不言自明的。首先，我们定义名为`<cronentries>`的根标签，在这些标签中，我们可以插入任意数量的`<cron>`标签——每一个标签表示一个预定的进程。在这些`<cron>`标签中，我们需要告诉调度程序我们想要点击的网址是什么(当然，这将是相对于根网址)，以及时间表本身(在我们的例子中，是每天上午 12:50)。其他可选标签包括描述标签、时区标签和/或目标标签，允许您指定调用指定 URL 的 GAE 项目版本。

现在，在我的例子中，我要求调度程序每天上午 12:50(PST)运行作业，但是其他调度格式的示例如下:

```java
every 12 hours
every 5 minutes from 10:00 to 14:00
2nd,third mon,wed,thu of march 17:00
every monday 09:00
1st monday of sep,oct,nov 17:00
every day 00:00

```

我不会讨论调度器标签的确切语法，但是您可以看到它非常直观。但是，如果您想了解更多关于 GAE CRON 工作的信息，或者想了解一些不常用的功能，请访问以下网址，全面了解 CRON 工作:

[http://code.google.com/appengine/docs/java/config/cron.html](http://code.google.com/appengine/docs/java/config/cron.html)

但是就我们的例子而言，我们之前所做的就足够了，所以我们就到此为止！

# 总结

在这一章中，我们又一次覆盖了很多领域。我们开始这一章只是简单地看了各种收集数据的方法。在某些情况下，其他公司发布的方便的 API 随时可供我们使用和查询(尽管在存储这些数据时必须小心法律问题)。然而，很多时候我们会发现自己需要出去自己获取这些数据，这可以通过网页抓取来实现。

在下一节中，我们学习了 web screen screen 的入门知识——从什么是 web screen screen 以及执行它需要采取什么步骤的高级概念开始，到实现结束。我们所经历的例子包括从 Blockbuster 的网站上抓取可供出租的最新视频游戏，在这个过程中，我们编写了第一个 XPath 表达式并实现了第一个 HTTP servlet。

在实现我们的 HTTP servlet 时，我们简要讨论了两种常见类型的 HTTP 请求(GET 和 POST 请求)，并继续实现一个 HTTP GET 请求，该请求允许我们调用我们的视频游戏刮板类，收集聚合的 `VideoGame`对象，然后使用我们上一章中方便的包装类将它们插入到我们的 JDO 数据库中。

最后，我们结束了这一章，看了我们可以安排 Blockbuster 的网站抓取的方法，以确保最新和最新的数据，而不必每天手动调用抓取器。我们引入了一种称为 CRON jobs 的特殊技术，并使用 GAE 平台实现了一种技术。

在下一章，也是最后一章，我们将努力把我们所学的一切结合起来。更具体地说，现在我们系统的数据收集和插入部分已经启动并运行，我们将再实现几个 servlets，允许我们发出 HTTP GET 请求并检索各种类型的数据。然后，我们将遍历代码的客户端，看看如何从安卓应用程序发出这些 GET 请求，并解析响应以获得所需的数据。